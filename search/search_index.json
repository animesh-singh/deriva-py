{
    "docs": [
        {
            "location": "/", 
            "text": "deriva-py Documentation\n\n\nThis site is the home for documentation about deriva-py, python APIs and CLIs (Command-Line Interfaces) for the DERIVA platform.", 
            "title": "deriva-py Documentation"
        }, 
        {
            "location": "/#deriva-py-documentation", 
            "text": "This site is the home for documentation about deriva-py, python APIs and CLIs (Command-Line Interfaces) for the DERIVA platform.", 
            "title": "deriva-py Documentation"
        }, 
        {
            "location": "/install/", 
            "text": "Installing deriva-py\n\n\nThis project is mostly in an early development phase. The \nmaster\n branch is expected to be stable and usable at every commit. The APIs and CLIs may change in backward-incompatible ways, so if you depend on an interface you should remember the GIT commit number.\n\n\nAt this time, we recommend installing from source, which can be accomplished with the \npip\n utility.\n\n\n$ pip install --upgrade git+https://github.com/informatics-isi-edu/deriva-py.git", 
            "title": "Installing deriva-py"
        }, 
        {
            "location": "/install/#installing-deriva-py", 
            "text": "This project is mostly in an early development phase. The  master  branch is expected to be stable and usable at every commit. The APIs and CLIs may change in backward-incompatible ways, so if you depend on an interface you should remember the GIT commit number.  At this time, we recommend installing from source, which can be accomplished with the  pip  utility.  $ pip install --upgrade git+https://github.com/informatics-isi-edu/deriva-py.git", 
            "title": "Installing deriva-py"
        }, 
        {
            "location": "/deriva-download-cli/", 
            "text": "deriva-download-cli\n\n\nThe \nderiva-download-cli\n is a command-line utility for orchestrating the bulk export of tabular data \n(stored in ERMRest) and download of asset data (stored in Hatrac, or other supported HTTP-accessible object store).\nIt supports the transfer of data directly to local filesystems, or packaging results into the\n\nbagit\n container format.  The program is driven by the combined\nusage of command-line arguments and a JSON-based configuration (\"spec\") file, which contains the processing\ndirectives used to orchestrate the creation of the result data set.\n\n\nFeatures\n\n\n\n\nTransfer both tabular data and file assets from Deriva catalogs.\n\n\nCreate \nbag\n containers, which may reference files stored in remote locations.\n\n\nSupports an extensible processing pipeline whereby data may be run through transform functions\nor other arbitrary processing before final result packaging.\n\n\n\n\nCommand-Line options\n\n\nusage: deriva-download-cli.py [-h] [--version] [--quiet] [--debug]\n                              [--credential-file \nfile\n] [--catalog \n1\n]\n                              [--token \nauth-token\n]\n                              \nhost\n \nconfig file\n \noutput dir\n ...\n\nDeriva Data Download Utility - CLI\n\npositional arguments:\n  \nhost\n                Fully qualified host name.\n  \nconfig file\n         Path to a configuration file.\n  \noutput dir\n          Path to an output directory.\n  [key=value key=value ...]\n                        Variable length of whitespace-delimited key=value pair\n                        arguments used for string interpolation in specific\n                        parts of the configuration file. For example:\n                        key1=value1 key2=value2\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             Print version and exit.\n  --quiet               Suppress logging output.\n  --debug               Enable debug logging output.\n  --credential-file \nfile\n\n                        Optional path to a credential file.\n  --catalog \n1\n         Catalog number. Default: 1\n  --token \nauth-token\n  Authorization bearer token.\n\n\n\n\nPositional arguments:\n\n\nhost\n\n\nAll operations are performed with respect to a specific host and most hosts will\nrequire authentication. If the \n--host HOSTNAME\n option is not given, \nlocalhost\n will be assumed.\n\n\nconfig file\n\n\nA path to a configuration file is required.  The format and syntax of the file is described \nbelow\n\n\noutput dir\n\n\nA path to a output base directory is required. This can be an absolute path or a path relative to the current working directory.\n\n\nOptional arguments:\n\n\n--token\n\n\nThe CLI accepts an authentication token with the \n--token TOKEN\n option. If this\noption is not given, it will look in the user home dir where the \nDERIVA-Auth\n\nclient would store the credentials.\n\n\n--credential-file\n\n\nIf \n--token\n is not specified, the program will look in the user home dir where the \nDERIVA-Auth\n\nclient would store the credentials.  Use the \n--credential file\n argument to override this behavior and specify an alternative credential file.\n\n\n--catalog\n\n\nThe catalog number (or path specifier). Defaults to 1.\n\n\n\nConfiguration file format\n\n\nThe configuration JSON file (or \"spec\") is the primary mechanism for orchestrating the export and download of data for a given host.\nThere are three primary objects that comprise the configuration spec; the \ncatalog\n element, the \nenv\n element, and the \nbag\n element.\n\n\nThe \ncatalog\n object is a REQUIRED element, and is principally composed of an array named \nqueries\n which is a set of configuration stanzas,\nexecuted in declared order, that individually describe \nwhat\n data to retrieve, \nhow\n the data should be processed, and \nwhere\n\nthe result data should be placed in the target filesystem.\n\n\nThe \nenv\n object is an OPTIONAL element which, if present, is expected to be a dictionary of key-value pairs that are available to use as\ninterpolation variables for various keywords in the \nqueries\n section of the configuration file.  The string substitution is performed using the keyword\ninterpolation syntax of Python \nstr.format\n.  NOTE: when specifying arbitrary\nkey-value pairs on the command-line, such pairs will OVERRIDE any matching keys found in the \nenv\n element of the configuration file.\n\n\nThe \nbag\n object is an OPTIONAL element which, if present, declares that the aggregate output from all configuration stanzas listed in the\n\ncatalog:queries\n array should be packaged as a \nbagit\n formatted container.  The \nbag\n element contains\nvarious optional parameters which control bag creation specifics.\n\n\nExample configuration file:\n\n\n{\n  \nenv\n: {\n    \naccession\n: \nXYZ123\n,\n    \nterm\n: \nChip-seq\n\n  },\n  \nbag\n: {\n    \nbag_name\n: \ntest-bag\n,\n    \nbag_archiver\n: \nzip\n,\n    \nbag_metadata\n: {\n      \nSource-Organization\n: \nUSC Information Sciences Institute, Informatics Systems Research Division\n\n    }\n  },\n  \ncatalog\n: {\n    \nqueries\n: [\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,techreplicate_num:=R:technical_replicate_number,species:=SPEC:term,paired:=PAIRED:term,stranded:=STRAND:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url\n,\n        \noutput_path\n: \n{accession}/{accession}-RNA-Seq\n,\n        \noutput_format\n: \ncsv\n\n      },\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:RID,experiment:=E:RID,biosample:=SAMPLE:RID,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url\n,\n        \noutput_path\n: \n{dataset}/{experiment}/{biosample}/seq\n,\n        \noutput_format\n: \ndownload\n\n      },\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,control:=E:control_assay,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,technical_replicate_num:=R:technical_replicate_number,species:=SPEC:term,target:=TARGET:term,paired:=PAIRED:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url\n,\n        \noutput_path\n: \n{accession}/{accession}-ChIP-Seq\n,\n        \noutput_format\n: \ncsv\n\n      },\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,technical_replicate_num:=R:technical_replicate_number,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url\n,\n        \noutput_path\n: \n{dataset}/{experiment}/{biosample}/seq\n,\n        \noutput_format\n: \nfetch\n\n      }\n    ]\n  }\n}\n\n\n\n\nConfiguration file element: \ncatalog\n\n\nExample:\n\n\n  \ncatalog\n: {\n    \nqueries\n: [\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,control:=E:control_assay,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,technical_replicate_num:=R:technical_replicate_number,species:=SPEC:term,target:=TARGET:term,paired:=PAIRED:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url\n,\n        \noutput_path\n: \n{accession}/{accession}-ChIP-Seq\n,\n        \noutput_format\n: \ncsv\n\n      },\n      {\n        \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,technical_replicate_num:=R:technical_replicate_number,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url\n,\n        \noutput_path\n: \n{dataset}/{experiment}/{biosample}/seq\n,\n        \noutput_format\n: \nfetch\n\n      }\n    ]\n  }\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nParent Object\n\n\nParameter\n\n\nDescription\n\n\nInterpolatable\n\n\n\n\n\n\n\n\n\n\nroot\n\n\ncatalog\n\n\nThis is the parent object for all catalog related parameters.\n\n\nNo\n\n\n\n\n\n\ncatalog\n\n\nqueries\n\n\nThis is an array of objects representing a list of \nERMRest\n queries and the logical outputs of these queries. The logical outputs of each query are then in turn processed by an \noutput format processor\n, which can either be one of a set of default processors, or an external class conforming to a specified interface.\n\n\nNo\n\n\n\n\n\n\nqueries\n\n\nquery_path\n\n\nThis is string representing the actual \nERMRest\n query path to be used in the HTTP(S) GET request. It SHOULD already be percent-encoded per \nRFC 3986\n if it contains any characters outside of the unreserved set.\n\n\nYes\n\n\n\n\n\n\nqueries\n\n\noutput_path\n\n\nThis is a POSIX-compliant path fragment indicating the target location of the retrieved data relative to the specified base download directory.\n\n\nYes\n\n\n\n\n\n\nqueries\n\n\noutput_format\n\n\nThis is a string value used to select from one of the built-in output processor formats. Valid values are \nenv\n, \ncsv\n, \njson\n, \njson-stream\n, \ndownload\n, or \nfetch\n.\n\n\nNo\n\n\n\n\n\n\nqueries\n\n\noutput_format_params\n\n\nA JSON object of arbitrary complexity containing format-specific processing instructions that will be passed to the \noutput_format_processor\n instance to be used. This is a context-specific parameter, i.e., this parameter MAY be required for a given \noutput_format_processor\n, otherwise it can be omitted.\n\n\nNo\n\n\n\n\n\n\nqueries\n\n\noutput_format_processor\n\n\nA fully qualified Python class name declaring an external processor class instance to use. If this parameter is present, it OVERRIDES the value (if specified) in \noutput_format\n. This class MUST be derived from the base class \nderiva.transfer.download.processors.BaseDownloadProcessor\n. For example, \n\"output_processor\": \"deriva.transfer.download.processors.CSVDownloadProcessor\"\n.\n\n\nNo\n\n\n\n\n\n\n\n\nConfiguration file element: \nbag\n\n\nExample:\n\n\nbag\n: {\n    \nbag_name\n: \ntest-bag\n,\n    \nbag_archiver\n: \nzip\n,\n    \nbag_algorithms\n: [\nsha256\n],\n    \nbag_metadata\n: {\n        \nSource-Organization\n: \nUSC Information Sciences Institute, Informatics Systems Research Division\n\n    }\n}\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nParent Object\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nroot\n\n\nbag\n\n\nThis is the parent object for all bag-related defaults.\n\n\n\n\n\n\nbag\n\n\nbag_algorithms\n\n\nThis is an array of strings representing the default checksum algorithms to use for bag manifests, if not otherwise specified.  Valid values are \"md5\", \"sha1\", \"sha256\", and \"sha512\".\n\n\n\n\n\n\nbag\n\n\nbag_archiver\n\n\nThis is a string representing the default archiving format to use if not otherwise specified.  Valid values are \"zip\", \"tar\", and \"tgz\".\n\n\n\n\n\n\nbag\n\n\nbag_metadata\n\n\nThis is a list of simple JSON key-value pairs that will be written as-is to bag-info.txt.\n\n\n\n\n\n\n\n\nConfiguration file element: \nenv\n\n\nExample:\n\n\nenv\n: {\n    \naccession\n: \nXYZ123\n,\n    \nterm\n: \nChip-seq\n\n}\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nParent Object\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nroot\n\n\nenv\n\n\nThis is the parent object for all global \"environment\" variables. Note that the usage of \n\"env\"\n in this case does not refer to the set of OS environment variables, but rather a combination of key-value pairs from the JSON configuration file and CLI arguments.\n\n\n\n\n\n\nenv\n\n\n[any, 0-N]\n\n\nAny number of arguments of the form \nkey:value\n where \nvalue\n is a \nstring\n.\n\n\n\n\n\n\n\n\nSupported output formats\n\n\nThe following \noutput_format\n tag values are supported by default:\n\n\n\n\n\n\n\n\nTag\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenv\n\n\nMetadata\n\n\nPopulates the context metadata (\"environment\") with values returned by the query.\n\n\n\n\n\n\ncsv\n\n\nCSV\n\n\nCSV format with column header row\n\n\n\n\n\n\njson\n\n\nJSON\n\n\nJSON Array of row objects.\n\n\n\n\n\n\njson-stream\n\n\n\"Streaming\" JSON\n\n\nNewline-delimited, multi-object JSON.\n\n\n\n\n\n\ndownload\n\n\nAsset download\n\n\nFile assets referenced by URL are download to local storage relative to \noutput_path\n.\n\n\n\n\n\n\nfetch\n\n\nAsset reference\n\n\nBag\n-based. File assets referenced by URL are assigned as remote file references via \nfetch.txt\n.\n\n\n\n\n\n\n\n\nOutput format details\n\n\nEach \noutput format processor\n is designed for a specific task, and the task types may vary for a given data export task.\nSome \noutput formats\n are designed to handle the export of tabular data from the catalog, while others are meant to handle the export of file assets that are referenced by tables in the catalog.\nOther \noutput formats\n may be implemented that could perform a combination of these tasks, implement a new format, or perform some kind of data transformation.\n\n\n\n\nenv\n\n\nThis \noutput_format\n processor performs a catalog query in JSON mode and stores the key-value pairs of the \nfirst\n row of data returned into the metadata context or \"working environment\" for the download.\nThese key-value pairs can then be used as interpolation variables in subsequent stages of processing.\n\n\n\n\ncsv\n\n\nThis \noutput_format\n generates a standard Comma Separated Values formatted text file. The first row is a comma-delimited list of column names, and all subsequent rows are comma-delimted values.  Fields are not enclosed in quotation marks.\n\n\nExample output:\n\n\nsubject_id,sample_id,snp_id,gt,chipset\nCNP0001_F09,600009963128,rs6265,0/1,HumanOmniExpress\nCNP0002_F15,600018902293,rs6265,0/0,HumanOmniExpress\n\n\n\n\n\n\njson\n\n\nThis \noutput_format\n generates a text file containing a JSON Array of row data, where each JSON object in the array represents one row.\n\n\nExample output:\n\n\n[{\nsubject_id\n:\nCNP0001_F09\n,\nsample_id\n:\n600009963128\n,\nsnp_id\n:\nrs6265\n,\ngt\n:\n0/1\n,\nchipset\n:\nHumanOmniExpress\n},\n {\nsubject_id\n:\nCNP0002_F15\n,\nsample_id\n:\n600018902293\n,\nsnp_id\n:\nrs6265\n,\ngt\n:\n0/0\n,\nchipset\n:\nHumanOmniExpress\n}]\n ```\n\n\na name=\njsons\n/a\n\n### `json-stream`\nThis `output_format` generates a text file containing multiple lines of individual JSON objects terminated by the _newline_ line terminator `\\n`. This\nformat is generally used when the result set is too prohibitively large to parse as a single JSON object and instead can be processed on a line-by-line basis.\n\nExample output:\n```json\n{\nsubject_id\n:\nCNP0001_F09\n,\nsample_id\n:\n600009963128\n,\nsnp_id\n:\nrs6265\n,\ngt\n:\n0/1\n,\nchipset\n:\nHumanOmniExpress\n}\n{\nsubject_id\n:\nCNP0002_F15\n,\nsample_id\n:\n600018902293\n,\nsnp_id\n:\nrs6265\n,\ngt\n:\n0/0\n,\nchipset\n:\nHumanOmniExpress\n}\n\n\n\n\n\n\ndownload\n\n\nThis \noutput_format\n processor performs multiple actions. First, it issues a \njson-stream\n catalog query against the specified \nquery_path\n,\nin order to generate a \nfile download manifest\n file named \ndownload-manifest.json\n. This manifest is simply a set of rows which MUST contain at least one field named \nurl\n, and MAY contain a field named \nfilename\n,\nand MAY contain other abritrary fields. If the \nfilename\n field is present, it will be appended to the final (calculated) \noutput_path\n, otherwise the application will perform a \nHEAD\n HTTP request against\nthe \nurl\n for the \nContent-Disposition\n of the referenced file asset. If this query fails to determine the filename, the application falls back to using the final string component of the \nurl\n field after the last \n/\n character.\n\n\nIf other fields are present, they are available for variable substitution in other parameters that support interpolation, e.g., \noutput_path\n.\n\n\nAfter the \nfile download manifest\n is generated, the application attempts to download the files referenced in each \nurl\n field to the local filesystem, storing them at the base relative path specified by \noutput_path\n.\n\n\nFor example, the following configuration stanza:\n\n\n{\n  \nquery_path\n: \n/attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:RID,experiment:=E:RID,biosample:=SAMPLE:RID,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url\n,\n  \noutput_path\n: \n{dataset}/{experiment}/{biosample}/seq\n,\n  \noutput_format\n: \ndownload\n\n}\n\n\n\n\nProduces a \ndownload-manifest.json\n with rows like:\n\n\n{\n  \ndataset\n:13641,\n  \nexperiment\n:51203,\n  \nbiosample\n:50233,\n  \nfile\n:55121,\n  \nfilename\n:\nLPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz\n,\n  \nsize\n:2976697043,\n  \nmd5\n:\n9139b1626a35122fa85688cbb7ae6a8a\n,\n  \nurl\n:\n/hatrac/facebase/data/fb2/FB00000806.2/LPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz\n\n}\n\n\n\n\nAfter the \noutput_path\n template string is interpolated with the values of the example row above, the file is then downloaded to the following relative path:\n\n\n./13641/51203/50233/seq/LPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz\n\n\n\n\n\n\nfetch\n\n\nThis \noutput_format\n processor performs multiple actions. First, it issues a \njson-stream\n catalog query against the specified \nquery_path\n, in order to generate a  \nfile download manifest\n.\nThis manifest is simply a set of rows which MUST contain at least one field named \nurl\n, and SHOULD contain two additional fields: \nlength\n,\nwhich is the size of the referenced file in bytes, and (at least) one of the following \nchecksum\n fields; \nmd5\n, \nsha1\n, \nsha256\n, \nsha512\n. If the \nlength\n and appropriate \nchecksum\n fields are missing,\nan attempt will be made to dynamically determine these fields from the remote \nurl\n by issuing a \nHEAD\n HTTP request and parsing the result headers for the missing information.\nIf the required values cannot be determined this way, it is an error condition and the transfer will abort.\n\n\nSimilar to the \ndownload\n processor, the output of the catalog query MAY contain other fields. If the \nfilename\n field is present, it will be appended to the final (calculated) \noutput_path\n, otherwise the application will perform a \nHEAD\n HTTP request against\nthe \nurl\n for the \nContent-Disposition\n of the referenced file asset. If this query fails to determine the filename, the application falls back to using the final name component of the \nurl\n field after the last \n/\n character.\nIf other fields are present, they are also available for variable substitution in other parameters that support interpolation, e.g., \noutput_path\n.\n\n\nUnlike the \ndownload\n processor, the \nfetch\n processor does not actually download any asset files, but rather uses the query results to create a \nbag\n with checksummed manifest entries that reference each remote asset via the \nbag\n's \nfetch.txt\n file.", 
            "title": "Bulk export tool (deriva-download-cli)"
        }, 
        {
            "location": "/deriva-download-cli/#deriva-download-cli", 
            "text": "The  deriva-download-cli  is a command-line utility for orchestrating the bulk export of tabular data \n(stored in ERMRest) and download of asset data (stored in Hatrac, or other supported HTTP-accessible object store).\nIt supports the transfer of data directly to local filesystems, or packaging results into the bagit  container format.  The program is driven by the combined\nusage of command-line arguments and a JSON-based configuration (\"spec\") file, which contains the processing\ndirectives used to orchestrate the creation of the result data set.", 
            "title": "deriva-download-cli"
        }, 
        {
            "location": "/deriva-download-cli/#features", 
            "text": "Transfer both tabular data and file assets from Deriva catalogs.  Create  bag  containers, which may reference files stored in remote locations.  Supports an extensible processing pipeline whereby data may be run through transform functions\nor other arbitrary processing before final result packaging.", 
            "title": "Features"
        }, 
        {
            "location": "/deriva-download-cli/#command-line-options", 
            "text": "usage: deriva-download-cli.py [-h] [--version] [--quiet] [--debug]\n                              [--credential-file  file ] [--catalog  1 ]\n                              [--token  auth-token ]\n                               host   config file   output dir  ...\n\nDeriva Data Download Utility - CLI\n\npositional arguments:\n   host                 Fully qualified host name.\n   config file          Path to a configuration file.\n   output dir           Path to an output directory.\n  [key=value key=value ...]\n                        Variable length of whitespace-delimited key=value pair\n                        arguments used for string interpolation in specific\n                        parts of the configuration file. For example:\n                        key1=value1 key2=value2\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             Print version and exit.\n  --quiet               Suppress logging output.\n  --debug               Enable debug logging output.\n  --credential-file  file \n                        Optional path to a credential file.\n  --catalog  1          Catalog number. Default: 1\n  --token  auth-token   Authorization bearer token.", 
            "title": "Command-Line options"
        }, 
        {
            "location": "/deriva-download-cli/#positional-arguments", 
            "text": "", 
            "title": "Positional arguments:"
        }, 
        {
            "location": "/deriva-download-cli/#host", 
            "text": "All operations are performed with respect to a specific host and most hosts will\nrequire authentication. If the  --host HOSTNAME  option is not given,  localhost  will be assumed.", 
            "title": "&lt;host&gt;"
        }, 
        {
            "location": "/deriva-download-cli/#config-file", 
            "text": "A path to a configuration file is required.  The format and syntax of the file is described  below", 
            "title": "&lt;config file&gt;"
        }, 
        {
            "location": "/deriva-download-cli/#output-dir", 
            "text": "A path to a output base directory is required. This can be an absolute path or a path relative to the current working directory.", 
            "title": "&lt;output dir&gt;"
        }, 
        {
            "location": "/deriva-download-cli/#optional-arguments", 
            "text": "", 
            "title": "Optional arguments:"
        }, 
        {
            "location": "/deriva-download-cli/#-token", 
            "text": "The CLI accepts an authentication token with the  --token TOKEN  option. If this\noption is not given, it will look in the user home dir where the  DERIVA-Auth \nclient would store the credentials.", 
            "title": "--token"
        }, 
        {
            "location": "/deriva-download-cli/#-credential-file", 
            "text": "If  --token  is not specified, the program will look in the user home dir where the  DERIVA-Auth \nclient would store the credentials.  Use the  --credential file  argument to override this behavior and specify an alternative credential file.", 
            "title": "--credential-file"
        }, 
        {
            "location": "/deriva-download-cli/#-catalog", 
            "text": "The catalog number (or path specifier). Defaults to 1.", 
            "title": "--catalog"
        }, 
        {
            "location": "/deriva-download-cli/#configuration-file-format", 
            "text": "The configuration JSON file (or \"spec\") is the primary mechanism for orchestrating the export and download of data for a given host.\nThere are three primary objects that comprise the configuration spec; the  catalog  element, the  env  element, and the  bag  element.  The  catalog  object is a REQUIRED element, and is principally composed of an array named  queries  which is a set of configuration stanzas,\nexecuted in declared order, that individually describe  what  data to retrieve,  how  the data should be processed, and  where \nthe result data should be placed in the target filesystem.  The  env  object is an OPTIONAL element which, if present, is expected to be a dictionary of key-value pairs that are available to use as\ninterpolation variables for various keywords in the  queries  section of the configuration file.  The string substitution is performed using the keyword\ninterpolation syntax of Python  str.format .  NOTE: when specifying arbitrary\nkey-value pairs on the command-line, such pairs will OVERRIDE any matching keys found in the  env  element of the configuration file.  The  bag  object is an OPTIONAL element which, if present, declares that the aggregate output from all configuration stanzas listed in the catalog:queries  array should be packaged as a  bagit  formatted container.  The  bag  element contains\nvarious optional parameters which control bag creation specifics.  Example configuration file:  {\n   env : {\n     accession :  XYZ123 ,\n     term :  Chip-seq \n  },\n   bag : {\n     bag_name :  test-bag ,\n     bag_archiver :  zip ,\n     bag_metadata : {\n       Source-Organization :  USC Information Sciences Institute, Informatics Systems Research Division \n    }\n  },\n   catalog : {\n     queries : [\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,techreplicate_num:=R:technical_replicate_number,species:=SPEC:term,paired:=PAIRED:term,stranded:=STRAND:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url ,\n         output_path :  {accession}/{accession}-RNA-Seq ,\n         output_format :  csv \n      },\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:RID,experiment:=E:RID,biosample:=SAMPLE:RID,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url ,\n         output_path :  {dataset}/{experiment}/{biosample}/seq ,\n         output_format :  download \n      },\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,control:=E:control_assay,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,technical_replicate_num:=R:technical_replicate_number,species:=SPEC:term,target:=TARGET:term,paired:=PAIRED:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url ,\n         output_path :  {accession}/{accession}-ChIP-Seq ,\n         output_format :  csv \n      },\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,technical_replicate_num:=R:technical_replicate_number,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url ,\n         output_path :  {dataset}/{experiment}/{biosample}/seq ,\n         output_format :  fetch \n      }\n    ]\n  }\n}", 
            "title": "Configuration file format"
        }, 
        {
            "location": "/deriva-download-cli/#configuration-file-element-catalog", 
            "text": "Example:     catalog : {\n     queries : [\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,control:=E:control_assay,biosample:=SAMPLE:RID,replicate:=R:RID,bioreplicate_num:=R:bioreplicate_number,technical_replicate_num:=R:technical_replicate_number,species:=SPEC:term,target:=TARGET:term,paired:=PAIRED:term,read:=SEQ:read,file:=SEQ:RID,filename:=SEQ:filename,url:=SEQ:url ,\n         output_path :  {accession}/{accession}-ChIP-Seq ,\n         output_format :  csv \n      },\n      {\n         query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=Chip-seq/$E/TARGET:=vocabulary:target_of_assay/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:accession,experiment:=E:RID,biosample:=SAMPLE:RID,technical_replicate_num:=R:technical_replicate_number,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url ,\n         output_path :  {dataset}/{experiment}/{biosample}/seq ,\n         output_format :  fetch \n      }\n    ]\n  }  Parameters:     Parent Object  Parameter  Description  Interpolatable      root  catalog  This is the parent object for all catalog related parameters.  No    catalog  queries  This is an array of objects representing a list of  ERMRest  queries and the logical outputs of these queries. The logical outputs of each query are then in turn processed by an  output format processor , which can either be one of a set of default processors, or an external class conforming to a specified interface.  No    queries  query_path  This is string representing the actual  ERMRest  query path to be used in the HTTP(S) GET request. It SHOULD already be percent-encoded per  RFC 3986  if it contains any characters outside of the unreserved set.  Yes    queries  output_path  This is a POSIX-compliant path fragment indicating the target location of the retrieved data relative to the specified base download directory.  Yes    queries  output_format  This is a string value used to select from one of the built-in output processor formats. Valid values are  env ,  csv ,  json ,  json-stream ,  download , or  fetch .  No    queries  output_format_params  A JSON object of arbitrary complexity containing format-specific processing instructions that will be passed to the  output_format_processor  instance to be used. This is a context-specific parameter, i.e., this parameter MAY be required for a given  output_format_processor , otherwise it can be omitted.  No    queries  output_format_processor  A fully qualified Python class name declaring an external processor class instance to use. If this parameter is present, it OVERRIDES the value (if specified) in  output_format . This class MUST be derived from the base class  deriva.transfer.download.processors.BaseDownloadProcessor . For example,  \"output_processor\": \"deriva.transfer.download.processors.CSVDownloadProcessor\" .  No", 
            "title": "Configuration file element: catalog"
        }, 
        {
            "location": "/deriva-download-cli/#configuration-file-element-bag", 
            "text": "Example:  bag : {\n     bag_name :  test-bag ,\n     bag_archiver :  zip ,\n     bag_algorithms : [ sha256 ],\n     bag_metadata : {\n         Source-Organization :  USC Information Sciences Institute, Informatics Systems Research Division \n    }\n}  Parameters:     Parent Object  Parameter  Description      root  bag  This is the parent object for all bag-related defaults.    bag  bag_algorithms  This is an array of strings representing the default checksum algorithms to use for bag manifests, if not otherwise specified.  Valid values are \"md5\", \"sha1\", \"sha256\", and \"sha512\".    bag  bag_archiver  This is a string representing the default archiving format to use if not otherwise specified.  Valid values are \"zip\", \"tar\", and \"tgz\".    bag  bag_metadata  This is a list of simple JSON key-value pairs that will be written as-is to bag-info.txt.", 
            "title": "Configuration file element: bag"
        }, 
        {
            "location": "/deriva-download-cli/#configuration-file-element-env", 
            "text": "Example:  env : {\n     accession :  XYZ123 ,\n     term :  Chip-seq \n}  Parameters:     Parent Object  Parameter  Description      root  env  This is the parent object for all global \"environment\" variables. Note that the usage of  \"env\"  in this case does not refer to the set of OS environment variables, but rather a combination of key-value pairs from the JSON configuration file and CLI arguments.    env  [any, 0-N]  Any number of arguments of the form  key:value  where  value  is a  string .", 
            "title": "Configuration file element: env"
        }, 
        {
            "location": "/deriva-download-cli/#supported-output-formats", 
            "text": "The following  output_format  tag values are supported by default:     Tag  Type  Description      env  Metadata  Populates the context metadata (\"environment\") with values returned by the query.    csv  CSV  CSV format with column header row    json  JSON  JSON Array of row objects.    json-stream  \"Streaming\" JSON  Newline-delimited, multi-object JSON.    download  Asset download  File assets referenced by URL are download to local storage relative to  output_path .    fetch  Asset reference  Bag -based. File assets referenced by URL are assigned as remote file references via  fetch.txt .", 
            "title": "Supported output formats"
        }, 
        {
            "location": "/deriva-download-cli/#output-format-details", 
            "text": "Each  output format processor  is designed for a specific task, and the task types may vary for a given data export task.\nSome  output formats  are designed to handle the export of tabular data from the catalog, while others are meant to handle the export of file assets that are referenced by tables in the catalog.\nOther  output formats  may be implemented that could perform a combination of these tasks, implement a new format, or perform some kind of data transformation.", 
            "title": "Output format details"
        }, 
        {
            "location": "/deriva-download-cli/#env", 
            "text": "This  output_format  processor performs a catalog query in JSON mode and stores the key-value pairs of the  first  row of data returned into the metadata context or \"working environment\" for the download.\nThese key-value pairs can then be used as interpolation variables in subsequent stages of processing.", 
            "title": "env"
        }, 
        {
            "location": "/deriva-download-cli/#csv", 
            "text": "This  output_format  generates a standard Comma Separated Values formatted text file. The first row is a comma-delimited list of column names, and all subsequent rows are comma-delimted values.  Fields are not enclosed in quotation marks.  Example output:  subject_id,sample_id,snp_id,gt,chipset\nCNP0001_F09,600009963128,rs6265,0/1,HumanOmniExpress\nCNP0002_F15,600018902293,rs6265,0/0,HumanOmniExpress", 
            "title": "csv"
        }, 
        {
            "location": "/deriva-download-cli/#json", 
            "text": "This  output_format  generates a text file containing a JSON Array of row data, where each JSON object in the array represents one row.  Example output:  [{ subject_id : CNP0001_F09 , sample_id : 600009963128 , snp_id : rs6265 , gt : 0/1 , chipset : HumanOmniExpress },\n { subject_id : CNP0002_F15 , sample_id : 600018902293 , snp_id : rs6265 , gt : 0/0 , chipset : HumanOmniExpress }]\n ``` a name= jsons /a \n### `json-stream`\nThis `output_format` generates a text file containing multiple lines of individual JSON objects terminated by the _newline_ line terminator `\\n`. This\nformat is generally used when the result set is too prohibitively large to parse as a single JSON object and instead can be processed on a line-by-line basis.\n\nExample output:\n```json\n{ subject_id : CNP0001_F09 , sample_id : 600009963128 , snp_id : rs6265 , gt : 0/1 , chipset : HumanOmniExpress }\n{ subject_id : CNP0002_F15 , sample_id : 600018902293 , snp_id : rs6265 , gt : 0/0 , chipset : HumanOmniExpress }", 
            "title": "json"
        }, 
        {
            "location": "/deriva-download-cli/#download", 
            "text": "This  output_format  processor performs multiple actions. First, it issues a  json-stream  catalog query against the specified  query_path ,\nin order to generate a  file download manifest  file named  download-manifest.json . This manifest is simply a set of rows which MUST contain at least one field named  url , and MAY contain a field named  filename ,\nand MAY contain other abritrary fields. If the  filename  field is present, it will be appended to the final (calculated)  output_path , otherwise the application will perform a  HEAD  HTTP request against\nthe  url  for the  Content-Disposition  of the referenced file asset. If this query fails to determine the filename, the application falls back to using the final string component of the  url  field after the last  /  character.  If other fields are present, they are available for variable substitution in other parameters that support interpolation, e.g.,  output_path .  After the  file download manifest  is generated, the application attempts to download the files referenced in each  url  field to the local filesystem, storing them at the base relative path specified by  output_path .  For example, the following configuration stanza:  {\n   query_path :  /attribute/D:=isa:dataset/accession={accession}/E:=isa:experiment/experiment_type:=isa:experiment_type/term=RNA%20expression%20%28RNA-seq%29/$E/STRAND:=vocabulary:strandedness/$E/R:=isa:replicate/SAMPLE:=isa:biosample/SPEC:=vocabulary:species/$R/SEQ:=isa:sequencing_data/PAIRED:=vocabulary:paired_end_or_single_read/$SEQ/file_format:=vocabulary:file_format/term=FastQ/$SEQ/dataset:=D:RID,experiment:=E:RID,biosample:=SAMPLE:RID,file:=SEQ:RID,filename:=SEQ:filename,size:=SEQ:byte_count,md5:=SEQ:md5,url:=SEQ:url ,\n   output_path :  {dataset}/{experiment}/{biosample}/seq ,\n   output_format :  download \n}  Produces a  download-manifest.json  with rows like:  {\n   dataset :13641,\n   experiment :51203,\n   biosample :50233,\n   file :55121,\n   filename : LPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz ,\n   size :2976697043,\n   md5 : 9139b1626a35122fa85688cbb7ae6a8a ,\n   url : /hatrac/facebase/data/fb2/FB00000806.2/LPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz \n}  After the  output_path  template string is interpolated with the values of the example row above, the file is then downloaded to the following relative path:  ./13641/51203/50233/seq/LPHW_111414_001A_e11.5_facebase_md_rna_R1.fastq.gz", 
            "title": "download"
        }, 
        {
            "location": "/deriva-download-cli/#fetch", 
            "text": "This  output_format  processor performs multiple actions. First, it issues a  json-stream  catalog query against the specified  query_path , in order to generate a   file download manifest .\nThis manifest is simply a set of rows which MUST contain at least one field named  url , and SHOULD contain two additional fields:  length ,\nwhich is the size of the referenced file in bytes, and (at least) one of the following  checksum  fields;  md5 ,  sha1 ,  sha256 ,  sha512 . If the  length  and appropriate  checksum  fields are missing,\nan attempt will be made to dynamically determine these fields from the remote  url  by issuing a  HEAD  HTTP request and parsing the result headers for the missing information.\nIf the required values cannot be determined this way, it is an error condition and the transfer will abort.  Similar to the  download  processor, the output of the catalog query MAY contain other fields. If the  filename  field is present, it will be appended to the final (calculated)  output_path , otherwise the application will perform a  HEAD  HTTP request against\nthe  url  for the  Content-Disposition  of the referenced file asset. If this query fails to determine the filename, the application falls back to using the final name component of the  url  field after the last  /  character.\nIf other fields are present, they are also available for variable substitution in other parameters that support interpolation, e.g.,  output_path .  Unlike the  download  processor, the  fetch  processor does not actually download any asset files, but rather uses the query results to create a  bag  with checksummed manifest entries that reference each remote asset via the  bag 's  fetch.txt  file.", 
            "title": "fetch"
        }, 
        {
            "location": "/deriva-hatrac-cli/", 
            "text": "deriva-hatrac-cli\n\n\nThe \nderiva-hatrac-cli\n is a command-line utility for interacting with the DERIVA \nHATRAC object store.\n\n\nFeatures\n\n\n\n\nList, create, and delete namespaces\n\n\nGet, put, and delete objects\n\n\nGet, set, and delete ACLs\n\n\n\n\nSee \nderiva-hatrac-cli --help\n for a complete list of its features, arguments and\noptions.\n\n\nCommon options\n\n\nAll operations are performed with respect to a specific host and most hosts will\nrequire authentication.\n\n\nHostname\n\n\nIf the \n--host HOSTNAME\n option is not given, \nlocalhost\n will be assumed.\n\n\nAuthentication\n\n\nThe CLI accepts an authentication token with the \n--token TOKEN\n option. If this \noption is not given, it will look in the user home dir where the \nDERIVA-Auth\n \nclient would store the credentials.\n\n\nNamespace operation examples\n\n\nList\n\n\n$ deriva-hatrac-cli --host example.org list /hatrac/\n/hatrac/path1\n/hatrac/path2\n/hatrac/path3\n\n\n\n\nCreate Namespace\n\n\n$ deriva-hatrac-cli --host example.org mkdir /hatrac/path1/foo\n\n\n\n\nDelete Namespace\n\n\n$ deriva-hatrac-cli --host example.org rmdir /hatrac/path1/foo\n\n\n\n\nHatrac does not allow reuse of namespace paths. If you create, delete, then \n(re)create a namespace you will get an error.\n\n\n$ deriva-hatrac-cli --host example.org mkdir /hatrac/path1/foo\nderiva-hatrac-cli mkdir: /hatrac/path1/foo: Namespace exists or the parent path is not a namespace\n\n\n\n\nObject operation examples\n\n\nPut an object\n\n\n$ deriva-hatrac-cli --host example.org put bar.jpg /hatrac/path1/foo/bar.jpg\n/hatrac/path1/foo/bar.jpg:LZJMSF6JQT7SFOVE2RBUZ4UEP4\n\n\n\n\nThe hatrac versioned path, ending in \"...\n:LZJMSF6JQT7SFOVE2RBUZ4UEP4\n\" is returned\non success.\n\n\nGet an object\n\n\n$ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg barcopy.jpg\n2017-11-28 12:15:40,700 - INFO - File [barcopy.jpg] transfer successful. 195.99 KB transferred at 29655.20 MB/second. Elapsed time: 0:00:00.006609.\n2017-11-28 12:15:40,700 - INFO - Verifying checksum for file [barcopy.jpg]\n\n\n\n\nIf the \noutfilename\n argument is not given it will take the \nbasename\n of the \nobject path.\n\n\n$ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg\n2017-11-28 12:17:03,236 - INFO - File [bar.jpg] transfer successful. 195.99 KB transferred at 30447.60 MB/second. Elapsed time: 0:00:00.006437.\n2017-11-28 12:17:03,236 - INFO - Verifying checksum for file [bar.jpg]\n\n\n\n\nAlternately, the object can be streamed to \nstdout\n which you could then pipe \nthrough another utility or redirect to a file as in this example.\n\n\n$ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg - \n barcopy2.jpg\n\n\n\n\nNote that when streaming to \nstdout\n the CLI will not (be able to) compute and \nverify the retrieved object's checksum.\n\n\nDelete an object\n\n\n$ deriva-hatrac-cli --host example.org del /hatrac/path1/foo/bar.jpg\n\n\n\n\nAs with namespaces, a deleted object path cannot be reused.\n\n\nACL operation examples\n\n\nACL operations may be performed on any hatrac \"path\"; i.e., on namespaces and \nobjects.\n\n\nGet ACLs\n\n\n$ deriva-hatrac-cli --host example.org getacl /hatrac/path1/foo\nsubtree-update:\ncreate:\nsubtree-create:\nsubtree-read:\nowner:\n  https://auth.globus.org/111111-2222-3333-4444-5555555\nsubtree-owner:\n\n\n\n\nSet ACLs\n\n\n$ deriva-hatrac-cli --host example.org setacl /hatrac/path1/foo subtree-read http://0000000-1111-2222-333333\n\n\n\n\nMore than one role may be added at a time.\n\n\nDelete ACLs\n\n\nDeleting an ACL deletes one or all \nrole\ns from a specified ACL \naccess-mode\n. See \nthe output of the \ngetacl\n operation for a list of all available \naccess-mode\ns.\n\n\n$ deriva-hatrac-cli --host example.org delacl /hatrac/path1/foo subtree-read", 
            "title": "Interact with Hatrac object store (deriva-hatrac-cli)"
        }, 
        {
            "location": "/deriva-hatrac-cli/#deriva-hatrac-cli", 
            "text": "The  deriva-hatrac-cli  is a command-line utility for interacting with the DERIVA \nHATRAC object store.", 
            "title": "deriva-hatrac-cli"
        }, 
        {
            "location": "/deriva-hatrac-cli/#features", 
            "text": "List, create, and delete namespaces  Get, put, and delete objects  Get, set, and delete ACLs   See  deriva-hatrac-cli --help  for a complete list of its features, arguments and\noptions.", 
            "title": "Features"
        }, 
        {
            "location": "/deriva-hatrac-cli/#common-options", 
            "text": "All operations are performed with respect to a specific host and most hosts will\nrequire authentication.", 
            "title": "Common options"
        }, 
        {
            "location": "/deriva-hatrac-cli/#hostname", 
            "text": "If the  --host HOSTNAME  option is not given,  localhost  will be assumed.", 
            "title": "Hostname"
        }, 
        {
            "location": "/deriva-hatrac-cli/#authentication", 
            "text": "The CLI accepts an authentication token with the  --token TOKEN  option. If this \noption is not given, it will look in the user home dir where the  DERIVA-Auth  \nclient would store the credentials.", 
            "title": "Authentication"
        }, 
        {
            "location": "/deriva-hatrac-cli/#namespace-operation-examples", 
            "text": "", 
            "title": "Namespace operation examples"
        }, 
        {
            "location": "/deriva-hatrac-cli/#list", 
            "text": "$ deriva-hatrac-cli --host example.org list /hatrac/\n/hatrac/path1\n/hatrac/path2\n/hatrac/path3", 
            "title": "List"
        }, 
        {
            "location": "/deriva-hatrac-cli/#create-namespace", 
            "text": "$ deriva-hatrac-cli --host example.org mkdir /hatrac/path1/foo", 
            "title": "Create Namespace"
        }, 
        {
            "location": "/deriva-hatrac-cli/#delete-namespace", 
            "text": "$ deriva-hatrac-cli --host example.org rmdir /hatrac/path1/foo  Hatrac does not allow reuse of namespace paths. If you create, delete, then \n(re)create a namespace you will get an error.  $ deriva-hatrac-cli --host example.org mkdir /hatrac/path1/foo\nderiva-hatrac-cli mkdir: /hatrac/path1/foo: Namespace exists or the parent path is not a namespace", 
            "title": "Delete Namespace"
        }, 
        {
            "location": "/deriva-hatrac-cli/#object-operation-examples", 
            "text": "", 
            "title": "Object operation examples"
        }, 
        {
            "location": "/deriva-hatrac-cli/#put-an-object", 
            "text": "$ deriva-hatrac-cli --host example.org put bar.jpg /hatrac/path1/foo/bar.jpg\n/hatrac/path1/foo/bar.jpg:LZJMSF6JQT7SFOVE2RBUZ4UEP4  The hatrac versioned path, ending in \"... :LZJMSF6JQT7SFOVE2RBUZ4UEP4 \" is returned\non success.", 
            "title": "Put an object"
        }, 
        {
            "location": "/deriva-hatrac-cli/#get-an-object", 
            "text": "$ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg barcopy.jpg\n2017-11-28 12:15:40,700 - INFO - File [barcopy.jpg] transfer successful. 195.99 KB transferred at 29655.20 MB/second. Elapsed time: 0:00:00.006609.\n2017-11-28 12:15:40,700 - INFO - Verifying checksum for file [barcopy.jpg]  If the  outfilename  argument is not given it will take the  basename  of the \nobject path.  $ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg\n2017-11-28 12:17:03,236 - INFO - File [bar.jpg] transfer successful. 195.99 KB transferred at 30447.60 MB/second. Elapsed time: 0:00:00.006437.\n2017-11-28 12:17:03,236 - INFO - Verifying checksum for file [bar.jpg]  Alternately, the object can be streamed to  stdout  which you could then pipe \nthrough another utility or redirect to a file as in this example.  $ deriva-hatrac-cli --host example.org get /hatrac/path1/foo/bar.jpg -   barcopy2.jpg  Note that when streaming to  stdout  the CLI will not (be able to) compute and \nverify the retrieved object's checksum.", 
            "title": "Get an object"
        }, 
        {
            "location": "/deriva-hatrac-cli/#delete-an-object", 
            "text": "$ deriva-hatrac-cli --host example.org del /hatrac/path1/foo/bar.jpg  As with namespaces, a deleted object path cannot be reused.", 
            "title": "Delete an object"
        }, 
        {
            "location": "/deriva-hatrac-cli/#acl-operation-examples", 
            "text": "ACL operations may be performed on any hatrac \"path\"; i.e., on namespaces and \nobjects.", 
            "title": "ACL operation examples"
        }, 
        {
            "location": "/deriva-hatrac-cli/#get-acls", 
            "text": "$ deriva-hatrac-cli --host example.org getacl /hatrac/path1/foo\nsubtree-update:\ncreate:\nsubtree-create:\nsubtree-read:\nowner:\n  https://auth.globus.org/111111-2222-3333-4444-5555555\nsubtree-owner:", 
            "title": "Get ACLs"
        }, 
        {
            "location": "/deriva-hatrac-cli/#set-acls", 
            "text": "$ deriva-hatrac-cli --host example.org setacl /hatrac/path1/foo subtree-read http://0000000-1111-2222-333333  More than one role may be added at a time.", 
            "title": "Set ACLs"
        }, 
        {
            "location": "/deriva-hatrac-cli/#delete-acls", 
            "text": "Deleting an ACL deletes one or all  role s from a specified ACL  access-mode . See \nthe output of the  getacl  operation for a list of all available  access-mode s.  $ deriva-hatrac-cli --host example.org delacl /hatrac/path1/foo subtree-read", 
            "title": "Delete ACLs"
        }, 
        {
            "location": "/deriva-acl-config/", 
            "text": "Using deriva-acl-config to configure ACLs\n\n\nThe \nderiva-acl-config\n utility reads a configuration file and uses it to set ACLs for an ermrest catalog (or for a schema or table within that catalog). Usage is:\n\n\nderiva-acl-config\n [\n-g\n|\n--groups-only\n] [\n-n\n|\n--dryrun\n] [\n-v\n|\n--verbose\n] [\n-s\n|\n--schema\n schema] [\n-t\n|\n--table\n table] [\n--host host\n] [\n--config-file\n config_file] [\n--credential-file\n credential_file] catalog\n\n\nwhere the required arguments are:\n\n\ncatalog\n: an ermrest catalog number (e.g., 1)\n\n\n--config_file\n file: the name of a configuration file\n\n\nOptions are:\n\n\n-credential_file\n file: read credentials from the named \nfile\n (if not specified, look for credentials maintained by \nderiva-auth\n)\n\n\n--groups-only\n: create and populate a group table (used for dynamic ACLs) based on the contents of the config file\n\n\n--dryrun\n: do nothing, just print out the catalog schema that would be applied\n\n\n--verbose\n: verbose, print acls and acl bindings for each object\n\n\n--schema\n schema: operate only on the named \nschema\n, not the whole catalog\n\n\n--table\n table: operate only on the named \ntable\n, not the whole catalog (requires the \n--schema\n option)\n\n\n--host\n host: configure the server on the specified \nhost\n (default \nlocalhost\n)\n\n\nConfig file format\n\n\nThe config file is a json file divided into the following stanzas:\n\n\ngroups\n: defines a set of named group lists, which can be used in ACL definitions later in the config file.\n\n\ngroup_list_table\n: the schema and table name of a table to populate with the information from the \ngroups\n stanza, so you can use the same named group lists in both static and dynamic ACLs and maintain them in one place. This table will typically be the last step in most dynamic ACL projections.\n\n\nacl_definitions\n: static ACL definitions (e.g., \"{\"write\": \"consortium\", \"select\": \"everyone\"}) that can be referred to later on in the config file (in this example, \nconsortium\n and \neveryone\n are group lists defined in the \ngroups\n stanza).\n\n\nacl_bindings\n: dynamic ACL definitions\n\n\ncatalog_acl\n: the ACL for the catalog; this will be one of the ACLs defined in the \nacl_definitions\n stanza.\n\n\nschema_acls\n: ACLs for individual schemas. Static ACLs (from \nacl_definitions\n) stanza are assigned to schemas.\n\n\ntable_acls\n: ACLs for individual tables. Static ACLs (from \nacl_definitions\n) and dynamic ACLs (from \nacl_bindings\n) are assigned to tabless.\n\n\ncolumn_acls\n: ACLs for individual columns. Static ACLs (from \nacl_definitions\n) and dynamic ACLs (from \nacl_bindings\n) are assigned to columns\n\n\nforeign_key_acls\n: ACLs for foreign keys. Static ACLs (from \nacl_definitions\n) and dynamic ACLs (from \nacl_bindings\n) are assigned to foreign keys\n\n\nThe groups stanza\n\n\nThe \ngroups\n stanza is a list of entries of the form \n\n\nname: [values]\n\n\nwhere \nname\n is a name that will be used to refer to a set of groups, and \nvalues\n is a list of group entries. The entries can be either the actual group IDs (from webauthn) or names of previously-defined groups. For example:\n\n\n    \ngroups\n : {\n    \nempty\n: [],\n    \npublic\n: [\n*\n],    \n        \nisrd-staff\n: [\nhttps://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b\n],\n        \nisrd-systems\n: [\nhttps://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b\n],\n        \nisrd-testers\n: [\nhttps://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d\n],\n    \nisrd-all\n: [\nisrd-staff\n, \nisrd-systems\n, \nisrd-testers\n]\n    }\n\n\n\n\nThe group_list_table stanza\n\n\nIf you're defining dynamic ACLs, at some point you'll probably want a table in your catalog somewhere that maps names to lists of groups. The group_list_table stanza specifies the schema and table name of a table to create for this list of groups. For example:\n\n\n    \ngroup_list_table\n : {\nschema\n : \n_acl_admin\n, \ntable\n : \ngroup_lists\n}\n\n\n\n\nThis will cause the \n_acl_admin.group_lists\n table to be created (if it doesn't already exist) and populated with the information specified in the \ngroups\n stanza. The \nname\n column of the table will be the primary key and will contain group name, and the \ngroups\n column will be the fully-expanded group list. The example \ngroups\n stanza above will create this table:\n\n\n     name     |                                                                                          groups                                                                                          \n--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n isrd-staff   | {https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b}\n isrd-testers | {https://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d}\n isrd-systems | {https://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b}\n isrd-all     | {https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b,https://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d,https://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b}\n public       | {*}\n empty        | {}\n\n\n\n\nThe acl_definitions stanza\n\n\nThis is where you define static ACLs for later use. The syntax is a list of\n\n\nname: value\n\n\nentries, where the \nname\n is a name you can refer to later, to assign these ACLs to objects, and the \nvalue\n is the ACL itself (which will probably contain references to the groups defined in the \ngroups\n stanza). For example:\n\n\n    \nacl_definitions\n : {\n        \nunrestricted_read\n : {\nselect\n : \npublic\n, \ncreate\n: \nisrd-systems\n, \nwrite\n: \nisrd-systems\n},\n        \nisrd_read\n : {\nselect\n : \nisrd-all\n, \ncreate\n: \nisrd-systems\n, \nwrite\n: \nisrd-systems\n},\n        \nsecret\n : {\nselect\n : \nempty\n}\n     }\n\n\n\n\nIn this example, the \nunrestricted_read\n ACL grants read access to everyone and restricts create and write access to the \nisrd-systems\n group; the \nisrd_read\n ACL is the same,except that it grants read access only to the \nisrd-all\n set of groups. Note that this stanza only defines the set of permissions and who they're associated with; by itself, it doesn't apply these ACLs to any object in the catalog.\n\n\nThe acl_bindings stanza\n\n\nThis is where you define dynamic ACLs for later use. The syntax is a list of\n\n\nname: value\n\n\npairs, where the \nname\n is a name you can refer to later, and the \nvalue\n is a dynamic ACL. For example:\n\n\n    \nacl_bindings\n : {\n    \na_binding\n : {\n        \nscope_acl\n: \nisrd-staff\n,\n        \ntypes\n : [\nselect\n],\n        \nprojection\n : [{\noutbound_col\n : \nallowed_groups\n}, \ngroups\n],\n        \nprojection_type\n : \nacl\n\n    }\n\n\n\n\nThis defines an ACL binding called \na_binding\n. The syntax of the binding itself is the same as defined in the ermrest ACL docs, with one exception: to specify an outbound foreign key, you can either use ermrest-standard \noutbound\n syntax and use the constraint name, or you can use \noutbound_col\n and specify the name of a column on which a foreign key is defined. It will also populate the \nscope_acl\n based on the referenced group list. For example, if you apply the binding \na_binding\n to this table:\n\n\n     Column     | Type | Modifiers \n----------------+------+-----------\n my_data        | text | \n allowed_groups | text | not null\nForeign-key constraints:\n    \nmytable_allowed_groups_fkey\n FOREIGN KEY (allowed_groups) REFERENCES _acl_admin.group_lists(name)\n\n\n\n\nthen the actual ermrest dynamic ACL will be:\n\n\n        \na_binding\n : {\n            \nscope_acl\n: [\nhttps://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b\n],\n            \ntypes\n : [\nselect\n],\n            \nprojection\n : [{\noutbound\n : \nmytable_allowed_groups_fkey\n}, \ngroups\n],\n            \nprojection_type\n : \nacl\n\n        }\n\n\n\n\nThis can be useful if you want to apply the same dynamic ACL to mutliple tables, since each table's foreign key will have a different name by default.\n\n\nThe catalog_acls stanza\n\n\nThis is where ACLs for the catalog itself are set. (Note: there's a bootstrapping issue for new catalogs - since this tool uses ermrest, you need to already have permission on the catalog before you can set any new permissions). For example:\n\n\n    \ncatalog_acl\n : {\nacl\n : \nunrestricted_read\n}\n\n\n\n\nThis applies the \nunrestricted_read\n ACL defined above to the catalog.\n\n\nThe schema_acls stanza\n\n\nThis is where ACLs for schemas are set. The syntax is a list of entries of the form:\n\n\n{schema_descriptor: value, acl_descriptor: value}\n\n\nA schema_descriptor is either:\n\n\n\"schema\":\n \nschema_name\n\n\nor\n\n\n\"schema_pattern:\"\n \nregular_expression\n\n\nWhen setting permissions on a schema:\n\n if an exact \nschema\n match is found, the associated ACL is used (and any matching \nschema_pattern\n entries are ignored).\n\n If no exact \nschema\n match is found and exactly one matching \nschema_pattern\n entry is found, then that ACL is used.\n* If no exact \nschema\n match is found and more than one matching \nschema_pattern\n entry is found, then an error is thrown.\n\n\nAn acl descriptor has the form\n\n\n\"acl\":\n \nname_of_acl_defined_earlier\n\n\nor\n\n\n\"no_acl\": true\n\n\nIf an \nacl\n is specified, the named static ACL is expanded and applied to the schema. If \nno_acl\n is specified, no ACL is applied to the schema (and as a result, it inherits whatever permissions are set by the catalog ACL).\n\n\nFor example:\n\n\n    \nschema_acls\n : [\n    {\nschema\n : \nVocabulary\n, \nno_acl\n : \ntrue\n},\n    {\nschema\n : \nISRD_Internal\n, \nacl\n : \nisrd_read\n},\n        {\nschema_pattern\n : \n.*\n, \nacl\n: \nsecret\n}\n    ]\n\n\n\n\nThis, paired with the catalog_acl stanza above, allows anyone to read the Vocabulary schema and ISRD people to read the \"ISRD_Internal\". It forbids anyone from reading any other schema in the catalog.\n\n\nThe table_acls stanza\n\n\nThis is where ACLs for tables are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, [acl_descriptor], [acl_bindings_descriptor]}\nA schema descriptor has the same form as in the schema_acls stanza.\n\n\nA table_descriptor is either:\n\n\n\"table\":\n \ntable_name\n\n\nor\n\n\n\"table_pattern:\"\n \nregular_expression\n\n\nRegular expression matching is used:\n\n If an entry with an exact \nschema\n and \ntable\n match is found, the associated ACL is used (and any other matching entries are ignored).\n\n Otherwise, if entry with an exact \nschema\n match and exactly one \ntable_pattern\n match is found, that ACL is used.\n\n Otherwise, if exactly one entry with a \nschema_pattern\n and \ntable_pattern\n match is found, that ACL is used.\n\n If none of the above is true, and multiple matching entries are found, then an error is thrown.\n\n\nAn acl_descriptor is the same as defined above.\n\n\nAn \nacl_bindings_descriptor\n has the form:\n\n\n\"acl_bindings\":\n [list_of_bindings]`\n\n\nwhere \nlist_of_bindings\n is a list of ACL bindings defined in the acl_bindings stanza.\n\n\nThe column_acls stanza\n\n\nThis is where ACLs for columns are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, column_descriptor, [acl_descriptor], [acl_bindings_descriptor] [invalidate_bindings_descriptor]}\nThe schema, table, acl, and acl_bindings descriptors have the same form as above.\nThe column_descriptor is either:\n\n\n\"column\":\n \ntable_name\n\n\nor\n\n\n\"column_pattern:\"\n \nregular_expression\n\n\nRegular expression matching is used:\n\n If an entry with exact \nschema\n, \ntable\n, and \ncolumn\n matches is found, the associated ACL is used (and any other matching entries are ignored).\n\n Otherwise, if exactly one entry is found with \nschema\n, \ntable\n, and \ncolumn\n matches is found, that ACL is used\n* If multiple regular-expression matches are found and no exact match is found, an exception is thrown.\n\n\nThe i\nnvalidate_bindings\n descriptor has the form:\n\n\n\"invalidate_bindings\"\n: [list_of_bindings]\n\n\nwhere \nlist_of_bindings\n is a list of bindng names to invalidate (i.e., ACL bindings that were defined on the column's table but that should not be applied to the column).\n\n\nThe foreign_key_acls stanza\n\n\nThis is where ACLs for foreign keys are set. The syntax is a list of entries of the form:\n\n\n{schema_descriptor, table_descriptor, fkey_schema_descriptor, fkey_name_descriptor, [acl_descriptor], [acl_bindings_descriptor], [invalidate_bindings_descriptor]}\n\n\nThe schema, table, acl, acl_bindings, and invalidate_bindings descriptors have the same form as above.\nThe fkey_schema_descriptor is either:\n\n\n\"foreign_key_schema\":\n foreign_key_schema_name\n\n\nor\n\n\n\"foreign_key_schema_pattern\":\n regular_expression\n\n\nThe fkey_name_descriptor is either:\n\n\n\"foreign_key\":\n foreign_key_name\n\n\nor\n\n\n\"foreign_key_pattern\":\n regular_expression\n\n\nThese specify the foreign key schema and name. As with the column_acls stanza:\n\n If an exact match is found, it's used\n\n If no exact match is found and exactly one regular expression match is found, it's used.\n* If no exact match is found and more than one regular expression match is found, an error is thrown.\n\n\nSecurity Considerations\n\n\nThere's no guarantee of the order in which changes will be applied. For example, if your current state restricts access to a table, like this:\n\n\n   \ntable_acls\n : [\n       {\nschema\n : \nmyschema\n, \ntable\n : \nmytable\n, \nacl\n: \nrestricted_access\n}\n   ]\n\n\n\n\nand you decide to change to a configuration that restricts access only to one sensitive column in that table:\n\n\n   \ntable_acls\n : [\n       {\nschema\n : \nmyschema\n, \ntable\n : \nmytable\n, \nacl\n: \nopen_access\n}\n   ],\n   \ncolumn_acls\n : [\n       {\nschema\n : \nmyschema\n, \ntable\n : \nmytable\n, \ncolumn\n : \nsensitive_column\n, \nacl\n: \nrestricted_access\n}\n   ]\n\n\n\n\n\nthen it's possible that the change to the table ACL will occur before the change to the column ACL, temporarily exposing the table and all its columns. The solution is to run \nacl_config\n in two passes, first adding the new restrictions and then removing the old ones.", 
            "title": "Configure ACLs (deriva-acl-config)"
        }, 
        {
            "location": "/deriva-acl-config/#using-deriva-acl-config-to-configure-acls", 
            "text": "The  deriva-acl-config  utility reads a configuration file and uses it to set ACLs for an ermrest catalog (or for a schema or table within that catalog). Usage is:  deriva-acl-config  [ -g | --groups-only ] [ -n | --dryrun ] [ -v | --verbose ] [ -s | --schema  schema] [ -t | --table  table] [ --host host ] [ --config-file  config_file] [ --credential-file  credential_file] catalog  where the required arguments are:  catalog : an ermrest catalog number (e.g., 1)  --config_file  file: the name of a configuration file  Options are:  -credential_file  file: read credentials from the named  file  (if not specified, look for credentials maintained by  deriva-auth )  --groups-only : create and populate a group table (used for dynamic ACLs) based on the contents of the config file  --dryrun : do nothing, just print out the catalog schema that would be applied  --verbose : verbose, print acls and acl bindings for each object  --schema  schema: operate only on the named  schema , not the whole catalog  --table  table: operate only on the named  table , not the whole catalog (requires the  --schema  option)  --host  host: configure the server on the specified  host  (default  localhost )", 
            "title": "Using deriva-acl-config to configure ACLs"
        }, 
        {
            "location": "/deriva-acl-config/#config-file-format", 
            "text": "The config file is a json file divided into the following stanzas:  groups : defines a set of named group lists, which can be used in ACL definitions later in the config file.  group_list_table : the schema and table name of a table to populate with the information from the  groups  stanza, so you can use the same named group lists in both static and dynamic ACLs and maintain them in one place. This table will typically be the last step in most dynamic ACL projections.  acl_definitions : static ACL definitions (e.g., \"{\"write\": \"consortium\", \"select\": \"everyone\"}) that can be referred to later on in the config file (in this example,  consortium  and  everyone  are group lists defined in the  groups  stanza).  acl_bindings : dynamic ACL definitions  catalog_acl : the ACL for the catalog; this will be one of the ACLs defined in the  acl_definitions  stanza.  schema_acls : ACLs for individual schemas. Static ACLs (from  acl_definitions ) stanza are assigned to schemas.  table_acls : ACLs for individual tables. Static ACLs (from  acl_definitions ) and dynamic ACLs (from  acl_bindings ) are assigned to tabless.  column_acls : ACLs for individual columns. Static ACLs (from  acl_definitions ) and dynamic ACLs (from  acl_bindings ) are assigned to columns  foreign_key_acls : ACLs for foreign keys. Static ACLs (from  acl_definitions ) and dynamic ACLs (from  acl_bindings ) are assigned to foreign keys", 
            "title": "Config file format"
        }, 
        {
            "location": "/deriva-acl-config/#the-groups-stanza", 
            "text": "The  groups  stanza is a list of entries of the form   name: [values]  where  name  is a name that will be used to refer to a set of groups, and  values  is a list of group entries. The entries can be either the actual group IDs (from webauthn) or names of previously-defined groups. For example:       groups  : {\n     empty : [],\n     public : [ * ],    \n         isrd-staff : [ https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b ],\n         isrd-systems : [ https://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b ],\n         isrd-testers : [ https://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d ],\n     isrd-all : [ isrd-staff ,  isrd-systems ,  isrd-testers ]\n    }", 
            "title": "The groups stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-group_list_table-stanza", 
            "text": "If you're defining dynamic ACLs, at some point you'll probably want a table in your catalog somewhere that maps names to lists of groups. The group_list_table stanza specifies the schema and table name of a table to create for this list of groups. For example:       group_list_table  : { schema  :  _acl_admin ,  table  :  group_lists }  This will cause the  _acl_admin.group_lists  table to be created (if it doesn't already exist) and populated with the information specified in the  groups  stanza. The  name  column of the table will be the primary key and will contain group name, and the  groups  column will be the fully-expanded group list. The example  groups  stanza above will create this table:       name     |                                                                                          groups                                                                                          \n--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n isrd-staff   | {https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b}\n isrd-testers | {https://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d}\n isrd-systems | {https://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b}\n isrd-all     | {https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b,https://auth.globus.org/9d596ac6-22b9-11e6-b519-22000aef184d,https://auth.globus.org/3938e0d0-ed35-11e5-8641-22000ab4b42b}\n public       | {*}\n empty        | {}", 
            "title": "The group_list_table stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-acl_definitions-stanza", 
            "text": "This is where you define static ACLs for later use. The syntax is a list of  name: value  entries, where the  name  is a name you can refer to later, to assign these ACLs to objects, and the  value  is the ACL itself (which will probably contain references to the groups defined in the  groups  stanza). For example:       acl_definitions  : {\n         unrestricted_read  : { select  :  public ,  create :  isrd-systems ,  write :  isrd-systems },\n         isrd_read  : { select  :  isrd-all ,  create :  isrd-systems ,  write :  isrd-systems },\n         secret  : { select  :  empty }\n     }  In this example, the  unrestricted_read  ACL grants read access to everyone and restricts create and write access to the  isrd-systems  group; the  isrd_read  ACL is the same,except that it grants read access only to the  isrd-all  set of groups. Note that this stanza only defines the set of permissions and who they're associated with; by itself, it doesn't apply these ACLs to any object in the catalog.", 
            "title": "The acl_definitions stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-acl_bindings-stanza", 
            "text": "This is where you define dynamic ACLs for later use. The syntax is a list of  name: value  pairs, where the  name  is a name you can refer to later, and the  value  is a dynamic ACL. For example:       acl_bindings  : {\n     a_binding  : {\n         scope_acl :  isrd-staff ,\n         types  : [ select ],\n         projection  : [{ outbound_col  :  allowed_groups },  groups ],\n         projection_type  :  acl \n    }  This defines an ACL binding called  a_binding . The syntax of the binding itself is the same as defined in the ermrest ACL docs, with one exception: to specify an outbound foreign key, you can either use ermrest-standard  outbound  syntax and use the constraint name, or you can use  outbound_col  and specify the name of a column on which a foreign key is defined. It will also populate the  scope_acl  based on the referenced group list. For example, if you apply the binding  a_binding  to this table:       Column     | Type | Modifiers \n----------------+------+-----------\n my_data        | text | \n allowed_groups | text | not null\nForeign-key constraints:\n     mytable_allowed_groups_fkey  FOREIGN KEY (allowed_groups) REFERENCES _acl_admin.group_lists(name)  then the actual ermrest dynamic ACL will be:           a_binding  : {\n             scope_acl : [ https://auth.globus.org/176baec4-ed26-11e5-8e88-22000ab4b42b ],\n             types  : [ select ],\n             projection  : [{ outbound  :  mytable_allowed_groups_fkey },  groups ],\n             projection_type  :  acl \n        }  This can be useful if you want to apply the same dynamic ACL to mutliple tables, since each table's foreign key will have a different name by default.", 
            "title": "The acl_bindings stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-catalog_acls-stanza", 
            "text": "This is where ACLs for the catalog itself are set. (Note: there's a bootstrapping issue for new catalogs - since this tool uses ermrest, you need to already have permission on the catalog before you can set any new permissions). For example:       catalog_acl  : { acl  :  unrestricted_read }  This applies the  unrestricted_read  ACL defined above to the catalog.", 
            "title": "The catalog_acls stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-schema_acls-stanza", 
            "text": "This is where ACLs for schemas are set. The syntax is a list of entries of the form:  {schema_descriptor: value, acl_descriptor: value}  A schema_descriptor is either:  \"schema\":   schema_name  or  \"schema_pattern:\"   regular_expression  When setting permissions on a schema:  if an exact  schema  match is found, the associated ACL is used (and any matching  schema_pattern  entries are ignored).  If no exact  schema  match is found and exactly one matching  schema_pattern  entry is found, then that ACL is used.\n* If no exact  schema  match is found and more than one matching  schema_pattern  entry is found, then an error is thrown.  An acl descriptor has the form  \"acl\":   name_of_acl_defined_earlier  or  \"no_acl\": true  If an  acl  is specified, the named static ACL is expanded and applied to the schema. If  no_acl  is specified, no ACL is applied to the schema (and as a result, it inherits whatever permissions are set by the catalog ACL).  For example:       schema_acls  : [\n    { schema  :  Vocabulary ,  no_acl  :  true },\n    { schema  :  ISRD_Internal ,  acl  :  isrd_read },\n        { schema_pattern  :  .* ,  acl :  secret }\n    ]  This, paired with the catalog_acl stanza above, allows anyone to read the Vocabulary schema and ISRD people to read the \"ISRD_Internal\". It forbids anyone from reading any other schema in the catalog.", 
            "title": "The schema_acls stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-table_acls-stanza", 
            "text": "This is where ACLs for tables are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, [acl_descriptor], [acl_bindings_descriptor]}\nA schema descriptor has the same form as in the schema_acls stanza.  A table_descriptor is either:  \"table\":   table_name  or  \"table_pattern:\"   regular_expression  Regular expression matching is used:  If an entry with an exact  schema  and  table  match is found, the associated ACL is used (and any other matching entries are ignored).  Otherwise, if entry with an exact  schema  match and exactly one  table_pattern  match is found, that ACL is used.  Otherwise, if exactly one entry with a  schema_pattern  and  table_pattern  match is found, that ACL is used.  If none of the above is true, and multiple matching entries are found, then an error is thrown.  An acl_descriptor is the same as defined above.  An  acl_bindings_descriptor  has the form:  \"acl_bindings\":  [list_of_bindings]`  where  list_of_bindings  is a list of ACL bindings defined in the acl_bindings stanza.", 
            "title": "The table_acls stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-column_acls-stanza", 
            "text": "This is where ACLs for columns are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, column_descriptor, [acl_descriptor], [acl_bindings_descriptor] [invalidate_bindings_descriptor]}\nThe schema, table, acl, and acl_bindings descriptors have the same form as above.\nThe column_descriptor is either:  \"column\":   table_name  or  \"column_pattern:\"   regular_expression  Regular expression matching is used:  If an entry with exact  schema ,  table , and  column  matches is found, the associated ACL is used (and any other matching entries are ignored).  Otherwise, if exactly one entry is found with  schema ,  table , and  column  matches is found, that ACL is used\n* If multiple regular-expression matches are found and no exact match is found, an exception is thrown.  The i nvalidate_bindings  descriptor has the form:  \"invalidate_bindings\" : [list_of_bindings]  where  list_of_bindings  is a list of bindng names to invalidate (i.e., ACL bindings that were defined on the column's table but that should not be applied to the column).", 
            "title": "The column_acls stanza"
        }, 
        {
            "location": "/deriva-acl-config/#the-foreign_key_acls-stanza", 
            "text": "This is where ACLs for foreign keys are set. The syntax is a list of entries of the form:  {schema_descriptor, table_descriptor, fkey_schema_descriptor, fkey_name_descriptor, [acl_descriptor], [acl_bindings_descriptor], [invalidate_bindings_descriptor]}  The schema, table, acl, acl_bindings, and invalidate_bindings descriptors have the same form as above.\nThe fkey_schema_descriptor is either:  \"foreign_key_schema\":  foreign_key_schema_name  or  \"foreign_key_schema_pattern\":  regular_expression  The fkey_name_descriptor is either:  \"foreign_key\":  foreign_key_name  or  \"foreign_key_pattern\":  regular_expression  These specify the foreign key schema and name. As with the column_acls stanza:  If an exact match is found, it's used  If no exact match is found and exactly one regular expression match is found, it's used.\n* If no exact match is found and more than one regular expression match is found, an error is thrown.", 
            "title": "The foreign_key_acls stanza"
        }, 
        {
            "location": "/deriva-acl-config/#security-considerations", 
            "text": "There's no guarantee of the order in which changes will be applied. For example, if your current state restricts access to a table, like this:      table_acls  : [\n       { schema  :  myschema ,  table  :  mytable ,  acl :  restricted_access }\n   ]  and you decide to change to a configuration that restricts access only to one sensitive column in that table:      table_acls  : [\n       { schema  :  myschema ,  table  :  mytable ,  acl :  open_access }\n   ],\n    column_acls  : [\n       { schema  :  myschema ,  table  :  mytable ,  column  :  sensitive_column ,  acl :  restricted_access }\n   ]  then it's possible that the change to the table ACL will occur before the change to the column ACL, temporarily exposing the table and all its columns. The solution is to run  acl_config  in two passes, first adding the new restrictions and then removing the old ones.", 
            "title": "Security Considerations"
        }, 
        {
            "location": "/deriva-annotation-config/", 
            "text": "Using deriva-annotation-config to configure annotations\n\n\nThe \nderiva-annotation-config\n utility reads a configuration file and uses it to set annotationss for an ermrest catalog (or for a schema or table within that catalog). Usage is:\n\n\nderiva-annotation-config\n [\n-n\n|\n--dryrun\n] [\n-v\n|\n--verbose\n] [\n-s\n|\n--schema\n schema] [\n-t\n|\n--table\n table] [\n--host host\n] [\n--config-file\n config_file] [\n--credential-file\n credential_file] catalog\n\n\nwhere the required arguments are:\n\n\ncatalog\n: an ermrest catalog number (e.g., 1)\n\n\n--config_file\n file: the name of a configuration file\n\n\nOptions are:\n\n\n-credential_file\n file: read credentials from the named \nfile\n (if not specified, look for credentials maintained by \nderiva-auth\n)\n\n\n--dryrun\n: do nothing, just print out the catalog schema that would be applied\n\n\n--verbose\n: verbose, print acls and acl bindings for each object\n\n\n--schema\n schema: operate only on the named \nschema\n, not the whole catalog\n\n\n--table\n table: operate only on the named \ntable\n, not the whole catalog (requires the \n--schema\n option)\n\n\n--host\n host: configure the server on the specified \nhost\n (default \nlocalhost\n)\n\n\nConfig file format\n\n\nThe config file is a json file divided into the following stanzas:\n\n\nknown_attributes\n: Attributes (i.e., annotation URIs, such as \ntag:isrd.isi.edu,2016:display\n) managed through this config file\n\n\nschema_annotations\n: Catalog-level annotations.\n\n\nschema_annotations\n: Annotations for individual schemas.\n\n\ntable_annotations\n: Annotations for individual tables.\n\n\ncolumn_annotations\n: Annotations for individual columns.\n\n\nforeign_key_annotations\n: Annotations for foreign keys.\n\n\nThe \nknown_attributes\n stanza\n\n\nThis stanza contains parameters that control the behavior of \nderiva-annotation-config\n itself. This section has three sub-sections: \nmanaged\n, \nignored\n, and \nignore_all_unmanaged\n.\n\n\nThe \nmanaged\n section is a list of annotation types managed by \nderiva-annotation-config\n.\n\n\nThe \nignore_all_unmanaged\n section is a boolean: if \ntrue\n, \nderiva-annotation-config\n will leave any annotations not in the \nmanaged\n list unchanged. If \nfalse\n, \nderiva-annotation-config\n will clear any annotations not in the \nmanaged\n list.\n\n\nThe \nignored\n section is a list of annotation types that are recognized by \nderiva-annotation-config\n but that aren't created or updated by it, depending on the value of \nignore_all_unmanaged\n. If \nignore_all_unmanaged\n is \ntrue\n, the program leaves existing annotations of this type alone. If \nignore_all_unmanaged\n is \nfalse\n, the program removes existing annotation sof this type.\n\n\nExample:\n\n\n    \nknown_attributes\n: {\n        \nignore_all_unmanaged\n: false,\n        \nmanaged\n: [\n            \ntag:isrd.isi.edu,2016:column-display\n, \n            \ntag:isrd.isi.edu,2016:display\n, \n            \ntag:isrd.isi.edu,2016:foreign-key\n, \n            ...\n    ],\n    \nignored\n : [\n            \ncomment\n, \n            \ndescription\n, \n            \nfacetOrder\n\n    ]\n    }\n\n\n\n\nThis is a version one might use to remove all \ncomment\n, \ndescription\n, and \nfacetOrder\n annotations from a catalog. If \nignore_all_attributes\n were \ntrue\n, those annotations would be left unchanged.\n\n\nThe schema_annotations stanza\n\n\nThis is where annotations for schemas are set. The syntax is a list of entries of the form:\n\n\n{schema_descriptor: value, annotation_descriptor: value}\n\n\nA schema_descriptor is either:\n\n\n\"schema\":\n \nschema_name\n\n\nor\n\n\n\"schema_pattern:\"\n \nregular_expression\n\n\nWhen setting permissions on a schema:\n\n if an exact \nschema\n match is found, the associated ACL is used (and any matching \nschema_pattern\n entries are ignored).\n\n If no exact \nschema\n match is found and exactly one matching \nschema_pattern\n entry is found, then that ACL is used.\n* If no exact \nschema\n match is found and more than one matching \nschema_pattern\n entry is found, then an error is thrown.\n\n\nAn annotation_descriptor has the form:\n\n\n\"uri\":\n \nannotation_uri\n\n\n\"value\":\n: \nannotation_value\n\n\nFor example:\n\n\n        {\n            \nschema\n: \nVocabulary\n, \n            \nuri\n: \ntag:misd.isi.edu,2015:display\n, \n            \nvalue\n: {\n                \nname_style\n: {\n                    \ntitle_case\n: true, \n                    \nunderline_space\n: true\n                }\n            }\n        }\n\n\n\n\nThe table_annotations stanza\n\n\nThis is where annotationss for tables are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, annotation_desciptor}\nThe schema_descriptor and annotation_descriptor have the same form as above.\n\n\nA table_descriptor is either:\n\n\n\"table\":\n \ntable_name\n\n\nor\n\n\n\"table_pattern:\"\n \nregular_expression\n\n\nRegular expression matching is used:\n\n If an entry with an exact \nschema\n and \ntable\n match is found, the associated ACL is used (and any other matching entries are ignored).\n\n Otherwise, if entry with an exact \nschema\n match and exactly one \ntable_pattern\n match is found, that ACL is used.\n\n Otherwise, if exactly one entry with a \nschema_pattern\n and \ntable_pattern\n match is found, that ACL is used.\n\n If none of the above is true, and multiple matching entries are found, then an error is thrown.\n\n\nFor example:\n\n\n    \ntable_annotations\n: [\n        {\n            \nschema\n: \nVocabulary\n, \n            \ntable_pattern\n: \n.*_terms\n, \n            \nuri\n: \ntag:isrd.isi.edu,2016:table-display\n, \n            \nvalue\n: {\n                \nrow_name\n: {\n                    \nrow_markdown_pattern\n: \n{{name}}\n\n                }\n            }\n        }\n\n\n\n\nThis sets an annotation for any table in the \"Vocabulary\" schema whose table name ends in \"_terms\".\n\n\nThe column_annotations stanza\n\n\nThis is where ACLs for columns are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, column_descriptor, annotation_descriptor}\nThe schema, table, and annotation descriptors have the same form as above.\nThe column_descriptor is either:\n\n\n\"column\":\n \ntable_name\n\n\nor\n\n\n\"column_pattern:\"\n \nregular_expression\n\n\nRegular expression matching is used:\n\n If an entry with exact \nschema\n, \ntable\n, and \ncolumn\n matches is found, the associated ACL is used (and any other matching entries are ignored).\n\n Otherwise, if exactly one entry is found with \nschema\n, \ntable\n, and \ncolumn\n matches is found, that ACL is used\n* If multiple regular-expression matches are found and no exact match is found, an exception is thrown.\n\n\nExample:\n\n\n        {\n            \ncolumn\n: \nRCT\n, \n            \nschema_pattern\n: \n.*\n, \n            \ntable_pattern\n: \n.*\n, \n            \nuri\n: \ntag:misd.isi.edu,2015:display\n, \n            \nvalue\n: {\n                \nname\n: \nCreation Time\n\n            }\n        }\n\n\n\n\nThis sets the display name for any column named \"RCT\" in the catalog.\n\n\nThe foreign_key_annotations stanza\n\n\nThis is where annotations for foreign keys are set. The syntax is a list of entries of the form:\n\n\n{schema_descriptor, table_descriptor, fkey_schema_descriptor, fkey_name_descriptor, annotation_descriptor}\n\n\nThe schema, table, and annotation descriptors have the same form as above.\nThe fkey_schema_descriptor is either:\n\n\n\"foreign_key_schema\":\n foreign_key_schema_name\n\n\nor\n\n\n\"foreign_key_schema_pattern\":\n regular_expression\n\n\nThe fkey_name_descriptor is either:\n\n\n\"foreign_key\":\n foreign_key_name\n\n\nor\n\n\n\"foreign_key_pattern\":\n regular_expression\n\n\nThese specify the foreign key schema and name. As with the column_acls stanza:\n\n If an exact match is found, it's used\n\n If no exact match is found and exactly one regular expression match is found, it's used.\n* If no exact match is found and more than one regular expression match is found, an error is thrown.\n\n\nExample:\n\n\n        {\n            \nforeign_key\n: \nAntibody_Gene_Association_NCBI_GeneID_fkey\n, \n            \nforeign_key_schema\n: \n, \n            \nschema\n: \nAntibody\n, \n            \ntable\n: \nAntibody_Gene_Association\n, \n            \nuri\n: \ntag:isrd.isi.edu,2016:foreign-key\n, \n            \nvalue\n: {\n                \nto_name\n: \nRelated Genes\n\n            }\n        }", 
            "title": "Configure annotations (deriva-annotation-config)"
        }, 
        {
            "location": "/deriva-annotation-config/#using-deriva-annotation-config-to-configure-annotations", 
            "text": "The  deriva-annotation-config  utility reads a configuration file and uses it to set annotationss for an ermrest catalog (or for a schema or table within that catalog). Usage is:  deriva-annotation-config  [ -n | --dryrun ] [ -v | --verbose ] [ -s | --schema  schema] [ -t | --table  table] [ --host host ] [ --config-file  config_file] [ --credential-file  credential_file] catalog  where the required arguments are:  catalog : an ermrest catalog number (e.g., 1)  --config_file  file: the name of a configuration file  Options are:  -credential_file  file: read credentials from the named  file  (if not specified, look for credentials maintained by  deriva-auth )  --dryrun : do nothing, just print out the catalog schema that would be applied  --verbose : verbose, print acls and acl bindings for each object  --schema  schema: operate only on the named  schema , not the whole catalog  --table  table: operate only on the named  table , not the whole catalog (requires the  --schema  option)  --host  host: configure the server on the specified  host  (default  localhost )", 
            "title": "Using deriva-annotation-config to configure annotations"
        }, 
        {
            "location": "/deriva-annotation-config/#config-file-format", 
            "text": "The config file is a json file divided into the following stanzas:  known_attributes : Attributes (i.e., annotation URIs, such as  tag:isrd.isi.edu,2016:display ) managed through this config file  schema_annotations : Catalog-level annotations.  schema_annotations : Annotations for individual schemas.  table_annotations : Annotations for individual tables.  column_annotations : Annotations for individual columns.  foreign_key_annotations : Annotations for foreign keys.", 
            "title": "Config file format"
        }, 
        {
            "location": "/deriva-annotation-config/#the-known_attributes-stanza", 
            "text": "This stanza contains parameters that control the behavior of  deriva-annotation-config  itself. This section has three sub-sections:  managed ,  ignored , and  ignore_all_unmanaged .  The  managed  section is a list of annotation types managed by  deriva-annotation-config .  The  ignore_all_unmanaged  section is a boolean: if  true ,  deriva-annotation-config  will leave any annotations not in the  managed  list unchanged. If  false ,  deriva-annotation-config  will clear any annotations not in the  managed  list.  The  ignored  section is a list of annotation types that are recognized by  deriva-annotation-config  but that aren't created or updated by it, depending on the value of  ignore_all_unmanaged . If  ignore_all_unmanaged  is  true , the program leaves existing annotations of this type alone. If  ignore_all_unmanaged  is  false , the program removes existing annotation sof this type.  Example:       known_attributes : {\n         ignore_all_unmanaged : false,\n         managed : [\n             tag:isrd.isi.edu,2016:column-display , \n             tag:isrd.isi.edu,2016:display , \n             tag:isrd.isi.edu,2016:foreign-key , \n            ...\n    ],\n     ignored  : [\n             comment , \n             description , \n             facetOrder \n    ]\n    }  This is a version one might use to remove all  comment ,  description , and  facetOrder  annotations from a catalog. If  ignore_all_attributes  were  true , those annotations would be left unchanged.", 
            "title": "The known_attributes stanza"
        }, 
        {
            "location": "/deriva-annotation-config/#the-schema_annotations-stanza", 
            "text": "This is where annotations for schemas are set. The syntax is a list of entries of the form:  {schema_descriptor: value, annotation_descriptor: value}  A schema_descriptor is either:  \"schema\":   schema_name  or  \"schema_pattern:\"   regular_expression  When setting permissions on a schema:  if an exact  schema  match is found, the associated ACL is used (and any matching  schema_pattern  entries are ignored).  If no exact  schema  match is found and exactly one matching  schema_pattern  entry is found, then that ACL is used.\n* If no exact  schema  match is found and more than one matching  schema_pattern  entry is found, then an error is thrown.  An annotation_descriptor has the form:  \"uri\":   annotation_uri  \"value\": :  annotation_value  For example:          {\n             schema :  Vocabulary , \n             uri :  tag:misd.isi.edu,2015:display , \n             value : {\n                 name_style : {\n                     title_case : true, \n                     underline_space : true\n                }\n            }\n        }", 
            "title": "The schema_annotations stanza"
        }, 
        {
            "location": "/deriva-annotation-config/#the-table_annotations-stanza", 
            "text": "This is where annotationss for tables are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, annotation_desciptor}\nThe schema_descriptor and annotation_descriptor have the same form as above.  A table_descriptor is either:  \"table\":   table_name  or  \"table_pattern:\"   regular_expression  Regular expression matching is used:  If an entry with an exact  schema  and  table  match is found, the associated ACL is used (and any other matching entries are ignored).  Otherwise, if entry with an exact  schema  match and exactly one  table_pattern  match is found, that ACL is used.  Otherwise, if exactly one entry with a  schema_pattern  and  table_pattern  match is found, that ACL is used.  If none of the above is true, and multiple matching entries are found, then an error is thrown.  For example:       table_annotations : [\n        {\n             schema :  Vocabulary , \n             table_pattern :  .*_terms , \n             uri :  tag:isrd.isi.edu,2016:table-display , \n             value : {\n                 row_name : {\n                     row_markdown_pattern :  {{name}} \n                }\n            }\n        }  This sets an annotation for any table in the \"Vocabulary\" schema whose table name ends in \"_terms\".", 
            "title": "The table_annotations stanza"
        }, 
        {
            "location": "/deriva-annotation-config/#the-column_annotations-stanza", 
            "text": "This is where ACLs for columns are set. The syntax is a list of entries of the form:\n{schema_descriptor, table_descriptor, column_descriptor, annotation_descriptor}\nThe schema, table, and annotation descriptors have the same form as above.\nThe column_descriptor is either:  \"column\":   table_name  or  \"column_pattern:\"   regular_expression  Regular expression matching is used:  If an entry with exact  schema ,  table , and  column  matches is found, the associated ACL is used (and any other matching entries are ignored).  Otherwise, if exactly one entry is found with  schema ,  table , and  column  matches is found, that ACL is used\n* If multiple regular-expression matches are found and no exact match is found, an exception is thrown.  Example:          {\n             column :  RCT , \n             schema_pattern :  .* , \n             table_pattern :  .* , \n             uri :  tag:misd.isi.edu,2015:display , \n             value : {\n                 name :  Creation Time \n            }\n        }  This sets the display name for any column named \"RCT\" in the catalog.", 
            "title": "The column_annotations stanza"
        }, 
        {
            "location": "/deriva-annotation-config/#the-foreign_key_annotations-stanza", 
            "text": "This is where annotations for foreign keys are set. The syntax is a list of entries of the form:  {schema_descriptor, table_descriptor, fkey_schema_descriptor, fkey_name_descriptor, annotation_descriptor}  The schema, table, and annotation descriptors have the same form as above.\nThe fkey_schema_descriptor is either:  \"foreign_key_schema\":  foreign_key_schema_name  or  \"foreign_key_schema_pattern\":  regular_expression  The fkey_name_descriptor is either:  \"foreign_key\":  foreign_key_name  or  \"foreign_key_pattern\":  regular_expression  These specify the foreign key schema and name. As with the column_acls stanza:  If an exact match is found, it's used  If no exact match is found and exactly one regular expression match is found, it's used.\n* If no exact match is found and more than one regular expression match is found, an error is thrown.  Example:          {\n             foreign_key :  Antibody_Gene_Association_NCBI_GeneID_fkey , \n             foreign_key_schema :  , \n             schema :  Antibody , \n             table :  Antibody_Gene_Association , \n             uri :  tag:isrd.isi.edu,2016:foreign-key , \n             value : {\n                 to_name :  Related Genes \n            }\n        }", 
            "title": "The foreign_key_annotations stanza"
        }, 
        {
            "location": "/api/", 
            "text": "Python APIs\n\n\nderiva-py\n includes Python APIs for interacting with the DERIVA framework.\n\n\nThe APIs include:\n\n\n\n\nlow-level ERMrest interface (see \nErmrestCatalog\n)\n\n\nlow-level Hatrac interface (see \nHatracStore\n)\n\n\nhigher-level ERMrest catalog configuration (see \nCatalogConfig\n)\n\n\nhigher-level ERMrest \"data path\" (see below)\n\n\n\n\nDataPath\n\n\nThe \ndatapath\n module is an interface for building ERMrest \"data paths\" and retrieving data from ERMrest catalogs. It\nalso supports data manipulation (insert, update, delete). In its present form, the module provides a limited \nprogrammatic interface to ERMrest.\n\n\nFeatures\n\n\n\n\nBuild ERMrest \"data path\" URLs with a Pythonic interface\n\n\nCovers the essentials for data retrieval: link tables, filter on attributes, select attributes, alias tables\n\n\nRetrieve entity sets; all or limited numbers of entities\n\n\nConvert entity sets to Pandas DataFrames\n\n\nInsert and update entities of a table\n\n\nDelete entities identified by a (potentially, complex) data path\n\n\n\n\nLimitations\n\n\n\n\nOnly supports \nentity\n and \nattribute\n resources\n\n\nOnly supports \napplication/json\n CONTENT-TYPE (i.e., protocol could be made more efficient)\n\n\nThe \nEntitySet\n interface is a thin wrapper over a dictionary of a list of results\n\n\nMany user errors are caught by Python \nassert\n statements rather than checking for \"invalid paramters\" and throwing\n  custom \nException\n objects\n\n\n\n\nTutorials\n\n\nSee the Jupyter Notebook tutorials in the \ndocs/\n folder.\n\n\n\n\nExample 1\n: basic schema inspection\n\n\nExample 2\n: basic data retrieval\n\n\nExample 3\n: building simple data paths\n\n\nExample 4\n: slightly more advanced topics\n\n\nData Update Example\n: examples of insert, update, and delete\n\n\n\n\nNow, \nget started\n!\n\n\nERMrest Model Management\n\n\nThe \ncore.ermrest_model\n module provides an interface for managing the\nmodel (schema definitions) of an ERMrest catalog.  This library\nprovides an (incomplete) set of helper routines for common model\nmanagement idioms.\n\n\nFor some advanced scenarios supported by the server but not yet\nsupported in this library, a client may need to resort to direct usage\nof the low-level \nderiva.core.ermrest_catalog.ErmrestCatalog\n HTTP\naccess layer.\n\n\nFeatures\n\n\n\n\nObtain an object hierarchy mirroring the model of a catalog or catalog snapshot.\n\n\nDiscover names of schemas, tables, and columns as well as definitions where applicable.\n\n\nDiscover names and definitions of key and foreign key constraints.\n\n\nDiscover annotations on catalog and model elements.\n\n\nDiscover policies on catalog and model elements (if sufficiently privileged).\n\n\nCreate model elements\n\n\nCreate new schemas.\n\n\nCreate new tables.\n\n\nCreate new columns on existing tables.\n\n\nCreate new key constraints over existing columns.\n\n\nCreate new foreign key constraints over existing columns and key constraints.\n\n\nDelete model elements\n\n\nDrop schemas.\n\n\nDrop tables.\n\n\nDrop columns.\n\n\nDrop key constraints.\n\n\nDrop foreign key constraints.\n\n\n\n\nLimitations\n\n\nBecause the model management interface mirrors a complex remote\ncatalog model with a hierarchy of local objects, it is possible for\nthe local objects to get out of synchronization with the remote\ncatalog and either represent model elements which no longer exist or\nlack model elements recently added.\n\n\nThe provided management methods, when used carefully, can\nincrementally update the local representation with changes made to the\nserver by the calling client. However, if other clients make\nconcurrent changes, it is likely that the local representation will\ndiverge.\n\n\nThe only robust solution to this problem is for the caller to discard\nits model representation, reconstruct it to match the latest server\nstate, and retry whatever changes are intended.\n\n\nExamples\n\n\nFor the following examples, we assume this common setup:\n\n\nfrom deriva.core import ErmrestCatalog\nimport deriva.core.ermrest_model as em\nfrom deriva.core.ermrest_model import builtin_types as typ\n\n\ncatalog = ErmrestCatalog(...)\nmodel_root = catalog.getCatalogModel()\n\n\n\nAlso, when examples show keyword arguments, they illustrate a typical\noverride value. If omitted, a default value will apply. Many parts of\nthe model definition are immutable once set, but in general \ncomment\n,\n\nacl\n, \nacl_binding\n, and \nannotation\n attributes can be modified after\nthe fact through configuration management APIs.\n\n\nAdd Table to Schema\n\n\nTo create a new table, you build a table definition document and pass\nit to the table-creation method on the object representing an existing\nschema. The various classes involved include class-methods\n\ndefine(...)\n to construct the constituent parts of the table\ndefinition:\n\n\ncolumn_defs = [ \n  em.Column.define(\"Col1\", typ.text), \n  em.Column.define(\"Col2\", typ.int8),\n]\nkey_defs = [\n  em.Key.define(\n    [\"Col1\"], # this is a list to allow for compound keys\n    constraint_names=[ [schema_name, \"My New Table_Col1_key\"] ],\n    comment=\"Col1 text values must be distinct.\",\n    annotations={},\n  )\n]\nfkey_defs = [\n  em.ForeignKey.define(\n    [\"Col2\"], # this is a list to allow for compound foreign keys\n    \"Foreign Schema\",\n    \"Referenced Table\",\n    [\"Referenced Column\"], # this is a list to allow for compound keys\n    on_update='CASCADE',\n    on_delete='SET NULL',\n    constraint_names=[ [schema_name, \"My New Table_Col2_fkey\"] ],\n    comment=\"Col2 must be a valid reference value from the domain table.\",\n    acls={},\n    acl_bindings={},\n    annotations={},\n  )\n]\ntable_def = em.Table.define(\n  \"My New Table\",\n  column_defs,\n  key_defs=key_defs,\n  fkey_defs=fkey_defs,\n  comment=\"My new entity type.\",\n  acls={},\n  acl_bindings={},\n  annotations={},\n  provide_system=True,\n)\nschema = model_root.schemas[schema_name]\nnew_table = schema.create_table(catalog, table_def)\n\n\n\nBy default, \ncreate_table(...)\n will add system columns to the table\ndefinition, so the caller does not need to reconstruct these standard elements\nof the column definitions nor the \nRID\n key definition.\n\n\nAdd a Vocabulary Term Table\n\n\nA vocabulary term table is often useful to track a controlled\nvocabulary used as a domain table for foreign key values used in\nscience data columns.  A simple vocabulary term table can be\ncreated with a helper function:\n\n\nschema = model_root.schemas[schema_name]\nnew_vocab_table = schema.create_table(catalog,\n  Table.define_vocabulary(\n    \"My Vocabulary\",\n    \"MYPROJECT:{RID}\",\n    \"https://server.example.org/id/{RID}\"\n  )\n)\n\n\n\nThe \nTable.define_vocabular()\n method is a convenience wrapper around\n\nTable.define()\n to automatically generate core vocabulary table\nstructures. It accepts other table definition parameters which a\nsophisticated caller can use to override or extend these core\nstructures.\n\n\nAdd Column to Table\n\n\nTo create a new column, you build a column definition document and\npass it to the column-creation method on the object representing an\nexisting table.\n\n\ncolumn_def = em.Column.define(\n  \"My New Column\",\n  typ.text,\n  nullok=False,\n  comment=\"A string representing my new stuff.\",\n  annotations={},\n  acls={},\n  acl_bindings={},\n)\ntable = model_root.table(schema_name, table_name)\nnew_column = table.create_column(catalog, column_def)\n\n\n\nThe same pattern can be used to add a key or foreign key to an\nexisting table via \ntable.create_key(catalog, key_def)\n or\n\ntable.create_fkey(catalog, fkey_def)\n, respectively. Similarly, a\nschema can be added to a model with \nmodel.create_schema(catalog,\nschema_def)\n.\n\n\nRemove a Column from a Table\n\n\nTo delete a column, you invoke the \ndelete()\n method on the\ncolumn object itself:\n\n\ntable = model_root.table(schema_name, table_name)\ncolumn = table.column_definitions[column_name]\ncolumn.delete(catalog, table=table)\n\n\n\nThe optional \ntable\n argument allows the method to prune the stale\nobject from the table object to reflect the change made on\nthe server. If this is omitted, the server change will be made but the\nlocal table object will fall out of synchronization.\n\n\nThe same pattern can be used to remove a key or foreign key from a\ntable via \nkey.delete(catalog, table)\n or \nforeign_key.delete(catalog,\ntable)\n, respectively. Similarly, a schema or table can be removed\nwith \nschema.delete(catalog, model)\n or \ntable.delete(catalog,\nschema)\n, respectively.\n\n\nderiva.core.ermrest_catalog\n\n\nThe \nderiva.core.ermrest_catalog.ErmrestCatalog\n class provides HTTP\nbindings to an ERMrest catalog as a thin wrapper around the Python\nRequests library.  Likewise, the\n\nderiva.core.ermrest_catalog.ErmrestSnapshot\n class provides HTTP\nbindings to an ERMrest catalog snapshot. While catalogs permit\nmutation of stored content, a snapshot is mostly read-only and only\npermits retrieval of content representing the state of the catalog at\na specific time in the past.\n\n\nInstances of \nErmrestCatalog\n or \nErmrestSnapshot\n represent a\nparticular remote catalog or catalog snapshot, respectively. They\nallow the client to perform HTTP requests against individual ERMrest\nresources, but require clients to know how to formulate those\nrequests in terms of URL paths and resource representations.\n\n\nOther, higher-level client APIs are layered on top of this implementation\nclass and exposed via factory-like methods integrated into each catalog\ninstance.\n\n\nCatalog Binding\n\n\nA catalog is bound using the class constructor, given parameters\nnecessary for binding:\n\n\nfrom deriva.core.ermrest_catalog import ErmrestCatalog\nfrom deriva.core import get_credential\n\nscheme = \"https\"\nserver = \"myserver.example.com\"\ncatalog_id = \"1\"\ncredentials = get_credential(server)\n\ncatalog = ErmrestCatalog(scheme, server, catalog_id, credentials=credentials)\n\n\n\nClient Credentials\n\n\nIn the preceding example, a credential is obtained from the filesystem\nassuming that the user has activated the \nderiva-auth\n authentication\nagent prior to executing this code. For catalogs allowing anonymous\naccess, the optional \ncredentials\n parameter can be omitted to\nestablish an anonymous binding.\n\n\nThe same client credentials (or anonymous access) is applied to all\nHTTP operations performed by the subsequent calls to the catalog\nobject's methods. If a calling program wishes to perform a mixture of\nrequests with different credentials, they should create multiple\ncatalog objects and choose the appropriate object for each request\nscenario.\n\n\nHigh-Level API Factories\n\n\nSeveral optional access APIs are layered on top of \nErmrestCatalog\n\nand/or \nErmrestSnapshot\n and may be accessed by invoking convenient\nfactory methods on a catalog or snapshot object:\n\n\n\n\ncatalog_snapshot = catalog.latest_snapshot()\n\n\nErmrestSnapshot\n binding for latest known revision of catalog\n\n\npath_builder = catalog.getPathBuilder()\n\n\nderiva.core.datapath.Catalog\n path builder for catalog (or snapshot)\n\n\nAllows higher-level data access idioms as described previously.\n\n\nconfig_root = catalog.getCatalogConfig()\n\n\nderiva.core.ermrest_config.CatalogConfig\n object for catalog (or snapshot)\n\n\nThe \nconfig_root\n object roots a tree of objects isomorphic to the catalog model, organizing configuration data according to each part of the model.\n\n\nAllows inspection of catalog/snapshot annotations and policies.\n\n\nAllows mutation to draft a new configuration objective.\n\n\nDraft changes are applied with \ncatalog.applyCatalogConfig(config_root)\n\n\nmodel_root = catalog.getCatalogModel()\n\n\nderiva.core.ermrest_model.Model\n object for catalog (or snapshot)\n\n\nThe \nmodel_root\n object roots a tree of objects isomorphic to the catalog model, organizing model definitions according to each part of the model.\n\n\nAllows inspection of catalog/snapshot models (schemas, tables, columns, constraints)\n\n\nSome model management idioms are exposed as methods on individual objects in the model hierarchy.\n\n\n\n\nLow-Level HTTP Methods\n\n\nWhen the client understands the URL structuring conventions of\nERMrest, they can use basic Python Requests idioms on a catalog\ninstance:\n\n\n\n\nresp = catalog.get(path)\n\n\nresp = catalog.delete(path)\n\n\nresp = catalog.put(path, json=data)\n\n\nresp = catalog.post(path, json=data)\n\n\n\n\nUnlike Python Requests, the \npath\n argument to each of these methods\nshould exclude the static prefix of the catalog itself. For example,\nassuming \ncatalog\n has been bound to\n\nhttps://myserver.example.com/ermrest/catalog/1\n as in the constructor\nexample above, an attempt to access table content at\n\nhttps://myserver.example.com/ermrest/catalog/1/entity/MyTable\n would\ncall \ncatalog.get(\n/entity/MyTable`) and the catalog binding would\nprepend the complete catalog prefix.\n\n\nThe \njson\n input to the \ncatalog.put\n and \ncatalog.post\n methods\nbehaves just as in Python Requests. The data is supplied as native\nPython lists, dictionaries, numbers, strings, and booleans. The method\nimplicitly serializes the data to JSON format and sets the appropriate\nContent-Type header to inform the server we are sending JSON content.\n\n\nAll of these HTTP methods return a \nrequests.Response\n object which\nmust be further interrogated to determine request status or to\nretrieve any content produced by the server:\n\n\n\n\nresp.status_code: the HTTP response status code\n\n\nresp.raise_for_status(): raise a Python exception for non-success codes\n\n\nresp.json(): deserialize JSON content from server response\n\n\nresp.headers: a dictionary of HTTP headers from the server response\n\n\n\n\nLow-level usage errors may raise exceptions directly from the HTTP\nmethods. However, normal server-indicated errors will produce a\nresponse object and the caller must interrogate the \nstatus_code\n\nfield or use the \nraise_for_status()\n helper to determine whether the\nrequest was successful.\n\n\nHTTP Caching\n\n\nBy default, the catalog binding uses HTTP caching for the\n\ncatalog.get\n method: it will store previous responses, include\nappropriate \nIf-None-Match\n headers in the new HTTP GET request,\ndetect \n304 Not Modified\n responses indicating that cached content is\nvalid, and return the cached content to the caller. This mechanism can\nbe disabled by specifying \ncaching=False\n in the ErmrestCatalog\nconstructor call.", 
            "title": "API Docs"
        }, 
        {
            "location": "/api/#python-apis", 
            "text": "deriva-py  includes Python APIs for interacting with the DERIVA framework.  The APIs include:   low-level ERMrest interface (see  ErmrestCatalog )  low-level Hatrac interface (see  HatracStore )  higher-level ERMrest catalog configuration (see  CatalogConfig )  higher-level ERMrest \"data path\" (see below)", 
            "title": "Python APIs"
        }, 
        {
            "location": "/api/#datapath", 
            "text": "The  datapath  module is an interface for building ERMrest \"data paths\" and retrieving data from ERMrest catalogs. It\nalso supports data manipulation (insert, update, delete). In its present form, the module provides a limited \nprogrammatic interface to ERMrest.", 
            "title": "DataPath"
        }, 
        {
            "location": "/api/#features", 
            "text": "Build ERMrest \"data path\" URLs with a Pythonic interface  Covers the essentials for data retrieval: link tables, filter on attributes, select attributes, alias tables  Retrieve entity sets; all or limited numbers of entities  Convert entity sets to Pandas DataFrames  Insert and update entities of a table  Delete entities identified by a (potentially, complex) data path", 
            "title": "Features"
        }, 
        {
            "location": "/api/#limitations", 
            "text": "Only supports  entity  and  attribute  resources  Only supports  application/json  CONTENT-TYPE (i.e., protocol could be made more efficient)  The  EntitySet  interface is a thin wrapper over a dictionary of a list of results  Many user errors are caught by Python  assert  statements rather than checking for \"invalid paramters\" and throwing\n  custom  Exception  objects", 
            "title": "Limitations"
        }, 
        {
            "location": "/api/#tutorials", 
            "text": "See the Jupyter Notebook tutorials in the  docs/  folder.   Example 1 : basic schema inspection  Example 2 : basic data retrieval  Example 3 : building simple data paths  Example 4 : slightly more advanced topics  Data Update Example : examples of insert, update, and delete   Now,  get started !", 
            "title": "Tutorials"
        }, 
        {
            "location": "/api/#ermrest-model-management", 
            "text": "The  core.ermrest_model  module provides an interface for managing the\nmodel (schema definitions) of an ERMrest catalog.  This library\nprovides an (incomplete) set of helper routines for common model\nmanagement idioms.  For some advanced scenarios supported by the server but not yet\nsupported in this library, a client may need to resort to direct usage\nof the low-level  deriva.core.ermrest_catalog.ErmrestCatalog  HTTP\naccess layer.", 
            "title": "ERMrest Model Management"
        }, 
        {
            "location": "/api/#features_1", 
            "text": "Obtain an object hierarchy mirroring the model of a catalog or catalog snapshot.  Discover names of schemas, tables, and columns as well as definitions where applicable.  Discover names and definitions of key and foreign key constraints.  Discover annotations on catalog and model elements.  Discover policies on catalog and model elements (if sufficiently privileged).  Create model elements  Create new schemas.  Create new tables.  Create new columns on existing tables.  Create new key constraints over existing columns.  Create new foreign key constraints over existing columns and key constraints.  Delete model elements  Drop schemas.  Drop tables.  Drop columns.  Drop key constraints.  Drop foreign key constraints.", 
            "title": "Features"
        }, 
        {
            "location": "/api/#limitations_1", 
            "text": "Because the model management interface mirrors a complex remote\ncatalog model with a hierarchy of local objects, it is possible for\nthe local objects to get out of synchronization with the remote\ncatalog and either represent model elements which no longer exist or\nlack model elements recently added.  The provided management methods, when used carefully, can\nincrementally update the local representation with changes made to the\nserver by the calling client. However, if other clients make\nconcurrent changes, it is likely that the local representation will\ndiverge.  The only robust solution to this problem is for the caller to discard\nits model representation, reconstruct it to match the latest server\nstate, and retry whatever changes are intended.", 
            "title": "Limitations"
        }, 
        {
            "location": "/api/#examples", 
            "text": "For the following examples, we assume this common setup:  from deriva.core import ErmrestCatalog\nimport deriva.core.ermrest_model as em\nfrom deriva.core.ermrest_model import builtin_types as typ\n\n\ncatalog = ErmrestCatalog(...)\nmodel_root = catalog.getCatalogModel()  Also, when examples show keyword arguments, they illustrate a typical\noverride value. If omitted, a default value will apply. Many parts of\nthe model definition are immutable once set, but in general  comment , acl ,  acl_binding , and  annotation  attributes can be modified after\nthe fact through configuration management APIs.", 
            "title": "Examples"
        }, 
        {
            "location": "/api/#add-table-to-schema", 
            "text": "To create a new table, you build a table definition document and pass\nit to the table-creation method on the object representing an existing\nschema. The various classes involved include class-methods define(...)  to construct the constituent parts of the table\ndefinition:  column_defs = [ \n  em.Column.define(\"Col1\", typ.text), \n  em.Column.define(\"Col2\", typ.int8),\n]\nkey_defs = [\n  em.Key.define(\n    [\"Col1\"], # this is a list to allow for compound keys\n    constraint_names=[ [schema_name, \"My New Table_Col1_key\"] ],\n    comment=\"Col1 text values must be distinct.\",\n    annotations={},\n  )\n]\nfkey_defs = [\n  em.ForeignKey.define(\n    [\"Col2\"], # this is a list to allow for compound foreign keys\n    \"Foreign Schema\",\n    \"Referenced Table\",\n    [\"Referenced Column\"], # this is a list to allow for compound keys\n    on_update='CASCADE',\n    on_delete='SET NULL',\n    constraint_names=[ [schema_name, \"My New Table_Col2_fkey\"] ],\n    comment=\"Col2 must be a valid reference value from the domain table.\",\n    acls={},\n    acl_bindings={},\n    annotations={},\n  )\n]\ntable_def = em.Table.define(\n  \"My New Table\",\n  column_defs,\n  key_defs=key_defs,\n  fkey_defs=fkey_defs,\n  comment=\"My new entity type.\",\n  acls={},\n  acl_bindings={},\n  annotations={},\n  provide_system=True,\n)\nschema = model_root.schemas[schema_name]\nnew_table = schema.create_table(catalog, table_def)  By default,  create_table(...)  will add system columns to the table\ndefinition, so the caller does not need to reconstruct these standard elements\nof the column definitions nor the  RID  key definition.", 
            "title": "Add Table to Schema"
        }, 
        {
            "location": "/api/#add-a-vocabulary-term-table", 
            "text": "A vocabulary term table is often useful to track a controlled\nvocabulary used as a domain table for foreign key values used in\nscience data columns.  A simple vocabulary term table can be\ncreated with a helper function:  schema = model_root.schemas[schema_name]\nnew_vocab_table = schema.create_table(catalog,\n  Table.define_vocabulary(\n    \"My Vocabulary\",\n    \"MYPROJECT:{RID}\",\n    \"https://server.example.org/id/{RID}\"\n  )\n)  The  Table.define_vocabular()  method is a convenience wrapper around Table.define()  to automatically generate core vocabulary table\nstructures. It accepts other table definition parameters which a\nsophisticated caller can use to override or extend these core\nstructures.", 
            "title": "Add a Vocabulary Term Table"
        }, 
        {
            "location": "/api/#add-column-to-table", 
            "text": "To create a new column, you build a column definition document and\npass it to the column-creation method on the object representing an\nexisting table.  column_def = em.Column.define(\n  \"My New Column\",\n  typ.text,\n  nullok=False,\n  comment=\"A string representing my new stuff.\",\n  annotations={},\n  acls={},\n  acl_bindings={},\n)\ntable = model_root.table(schema_name, table_name)\nnew_column = table.create_column(catalog, column_def)  The same pattern can be used to add a key or foreign key to an\nexisting table via  table.create_key(catalog, key_def)  or table.create_fkey(catalog, fkey_def) , respectively. Similarly, a\nschema can be added to a model with  model.create_schema(catalog,\nschema_def) .", 
            "title": "Add Column to Table"
        }, 
        {
            "location": "/api/#remove-a-column-from-a-table", 
            "text": "To delete a column, you invoke the  delete()  method on the\ncolumn object itself:  table = model_root.table(schema_name, table_name)\ncolumn = table.column_definitions[column_name]\ncolumn.delete(catalog, table=table)  The optional  table  argument allows the method to prune the stale\nobject from the table object to reflect the change made on\nthe server. If this is omitted, the server change will be made but the\nlocal table object will fall out of synchronization.  The same pattern can be used to remove a key or foreign key from a\ntable via  key.delete(catalog, table)  or  foreign_key.delete(catalog,\ntable) , respectively. Similarly, a schema or table can be removed\nwith  schema.delete(catalog, model)  or  table.delete(catalog,\nschema) , respectively.", 
            "title": "Remove a Column from a Table"
        }, 
        {
            "location": "/api/#derivacoreermrest_catalog", 
            "text": "The  deriva.core.ermrest_catalog.ErmrestCatalog  class provides HTTP\nbindings to an ERMrest catalog as a thin wrapper around the Python\nRequests library.  Likewise, the deriva.core.ermrest_catalog.ErmrestSnapshot  class provides HTTP\nbindings to an ERMrest catalog snapshot. While catalogs permit\nmutation of stored content, a snapshot is mostly read-only and only\npermits retrieval of content representing the state of the catalog at\na specific time in the past.  Instances of  ErmrestCatalog  or  ErmrestSnapshot  represent a\nparticular remote catalog or catalog snapshot, respectively. They\nallow the client to perform HTTP requests against individual ERMrest\nresources, but require clients to know how to formulate those\nrequests in terms of URL paths and resource representations.  Other, higher-level client APIs are layered on top of this implementation\nclass and exposed via factory-like methods integrated into each catalog\ninstance.", 
            "title": "deriva.core.ermrest_catalog"
        }, 
        {
            "location": "/api/#catalog-binding", 
            "text": "A catalog is bound using the class constructor, given parameters\nnecessary for binding:  from deriva.core.ermrest_catalog import ErmrestCatalog\nfrom deriva.core import get_credential\n\nscheme = \"https\"\nserver = \"myserver.example.com\"\ncatalog_id = \"1\"\ncredentials = get_credential(server)\n\ncatalog = ErmrestCatalog(scheme, server, catalog_id, credentials=credentials)", 
            "title": "Catalog Binding"
        }, 
        {
            "location": "/api/#client-credentials", 
            "text": "In the preceding example, a credential is obtained from the filesystem\nassuming that the user has activated the  deriva-auth  authentication\nagent prior to executing this code. For catalogs allowing anonymous\naccess, the optional  credentials  parameter can be omitted to\nestablish an anonymous binding.  The same client credentials (or anonymous access) is applied to all\nHTTP operations performed by the subsequent calls to the catalog\nobject's methods. If a calling program wishes to perform a mixture of\nrequests with different credentials, they should create multiple\ncatalog objects and choose the appropriate object for each request\nscenario.", 
            "title": "Client Credentials"
        }, 
        {
            "location": "/api/#high-level-api-factories", 
            "text": "Several optional access APIs are layered on top of  ErmrestCatalog \nand/or  ErmrestSnapshot  and may be accessed by invoking convenient\nfactory methods on a catalog or snapshot object:   catalog_snapshot = catalog.latest_snapshot()  ErmrestSnapshot  binding for latest known revision of catalog  path_builder = catalog.getPathBuilder()  deriva.core.datapath.Catalog  path builder for catalog (or snapshot)  Allows higher-level data access idioms as described previously.  config_root = catalog.getCatalogConfig()  deriva.core.ermrest_config.CatalogConfig  object for catalog (or snapshot)  The  config_root  object roots a tree of objects isomorphic to the catalog model, organizing configuration data according to each part of the model.  Allows inspection of catalog/snapshot annotations and policies.  Allows mutation to draft a new configuration objective.  Draft changes are applied with  catalog.applyCatalogConfig(config_root)  model_root = catalog.getCatalogModel()  deriva.core.ermrest_model.Model  object for catalog (or snapshot)  The  model_root  object roots a tree of objects isomorphic to the catalog model, organizing model definitions according to each part of the model.  Allows inspection of catalog/snapshot models (schemas, tables, columns, constraints)  Some model management idioms are exposed as methods on individual objects in the model hierarchy.", 
            "title": "High-Level API Factories"
        }, 
        {
            "location": "/api/#low-level-http-methods", 
            "text": "When the client understands the URL structuring conventions of\nERMrest, they can use basic Python Requests idioms on a catalog\ninstance:   resp = catalog.get(path)  resp = catalog.delete(path)  resp = catalog.put(path, json=data)  resp = catalog.post(path, json=data)   Unlike Python Requests, the  path  argument to each of these methods\nshould exclude the static prefix of the catalog itself. For example,\nassuming  catalog  has been bound to https://myserver.example.com/ermrest/catalog/1  as in the constructor\nexample above, an attempt to access table content at https://myserver.example.com/ermrest/catalog/1/entity/MyTable  would\ncall  catalog.get( /entity/MyTable`) and the catalog binding would\nprepend the complete catalog prefix.  The  json  input to the  catalog.put  and  catalog.post  methods\nbehaves just as in Python Requests. The data is supplied as native\nPython lists, dictionaries, numbers, strings, and booleans. The method\nimplicitly serializes the data to JSON format and sets the appropriate\nContent-Type header to inform the server we are sending JSON content.  All of these HTTP methods return a  requests.Response  object which\nmust be further interrogated to determine request status or to\nretrieve any content produced by the server:   resp.status_code: the HTTP response status code  resp.raise_for_status(): raise a Python exception for non-success codes  resp.json(): deserialize JSON content from server response  resp.headers: a dictionary of HTTP headers from the server response   Low-level usage errors may raise exceptions directly from the HTTP\nmethods. However, normal server-indicated errors will produce a\nresponse object and the caller must interrogate the  status_code \nfield or use the  raise_for_status()  helper to determine whether the\nrequest was successful.", 
            "title": "Low-Level HTTP Methods"
        }, 
        {
            "location": "/api/#http-caching", 
            "text": "By default, the catalog binding uses HTTP caching for the catalog.get  method: it will store previous responses, include\nappropriate  If-None-Match  headers in the new HTTP GET request,\ndetect  304 Not Modified  responses indicating that cached content is\nvalid, and return the cached content to the caller. This mechanism can\nbe disabled by specifying  caching=False  in the ErmrestCatalog\nconstructor call.", 
            "title": "HTTP Caching"
        }
    ]
}